{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "261ceb96",
   "metadata": {
    "tags": []
   },
   "source": [
    "# FINAL TA: Multi Label\n",
    "Name : linkv12 <br>\n",
    "Date : 25 November 2022<br>\n",
    "Finalize the experiment for single label <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09143fd4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524ac686",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, losses, Model, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6fb4866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os for stuff\n",
    "import os\n",
    "import re\n",
    "\n",
    "import copy\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "# for graphing\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import multilabel_confusion_matrix, hamming_loss, accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection  import KFold\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# to supress invalid warning \n",
    "old_err_state = np.seterr(invalid ='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b03e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk\n",
    "import nltk\n",
    "nltk.download(['stopwords','punkt'])\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "porterStemmer = nltk.stem.PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80ab40c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6e6d4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCHSIZE = 8\n",
    "EPOCH = 100\n",
    "EMBEDDING_DIM = 750\n",
    "\n",
    "SAVE_AE = False\n",
    "\n",
    "sep = '\\\\' if os.name == 'nt' else '/'\n",
    "cwd = os.getcwd()\n",
    "\n",
    "FOLDERNAME = r'{}{}data'.format(cwd, sep, sep)\n",
    "DATASETNAME = r'{}{}elsvier.csv'.format(FOLDERNAME, sep)\n",
    "\n",
    "RESULTS = r\"{}{}results{}\".format(cwd, sep, sep)\n",
    "\n",
    "COLUMN_NAME = ['abstract','subjareas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c77805d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cwd)\n",
    "print(FOLDERNAME)\n",
    "print(DATASETNAME)\n",
    "print(RESULTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff147d4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataset Wrangling\n",
    "Make sure the data type and formating is close to a final expected type<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93312cb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>subjareas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Models that estimate land-use impacts on biodi...</td>\n",
       "      <td>['AGRI', 'ENVI']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This paper investigates food waste dynamics in...</td>\n",
       "      <td>['ECON', 'ENVI']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Piwi-interacting RNAs (piRNAs) are important f...</td>\n",
       "      <td>['BIOC']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The demise of Miocene carbonate build-ups in t...</td>\n",
       "      <td>['EART']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Over recent decades, Land Use and Cover Change...</td>\n",
       "      <td>['ENVI']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39520</th>\n",
       "      <td>Background To date, there have been few studie...</td>\n",
       "      <td>['MEDI', 'PSYC']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39521</th>\n",
       "      <td>We argue that a hexagonal grid with simple int...</td>\n",
       "      <td>['COMP', 'MATH']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39522</th>\n",
       "      <td>Experimental and theoretical studies show that...</td>\n",
       "      <td>['AGRI']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39523</th>\n",
       "      <td>This review discusses recent developments in t...</td>\n",
       "      <td>['BIOC', 'PHAR']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39524</th>\n",
       "      <td>A system for the radiological protection of th...</td>\n",
       "      <td>['ENVI']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39525 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                abstract         subjareas\n",
       "0      Models that estimate land-use impacts on biodi...  ['AGRI', 'ENVI']\n",
       "1      This paper investigates food waste dynamics in...  ['ECON', 'ENVI']\n",
       "2      Piwi-interacting RNAs (piRNAs) are important f...          ['BIOC']\n",
       "3      The demise of Miocene carbonate build-ups in t...          ['EART']\n",
       "4      Over recent decades, Land Use and Cover Change...          ['ENVI']\n",
       "...                                                  ...               ...\n",
       "39520  Background To date, there have been few studie...  ['MEDI', 'PSYC']\n",
       "39521  We argue that a hexagonal grid with simple int...  ['COMP', 'MATH']\n",
       "39522  Experimental and theoretical studies show that...          ['AGRI']\n",
       "39523  This review discusses recent developments in t...  ['BIOC', 'PHAR']\n",
       "39524  A system for the radiological protection of th...          ['ENVI']\n",
       "\n",
       "[39525 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a single df \n",
    "df = pd.read_csv(DATASETNAME, usecols=COLUMN_NAME)\n",
    "\n",
    "\n",
    "# shuffle the DataFrame rows\n",
    "shuff = df.sample(frac = 1, random_state=4473, ignore_index=True)\n",
    "\n",
    "shuff\n",
    "# # label encoding\n",
    "# # creating instance of labelencoder\n",
    "# labelencoder = LabelEncoder()\n",
    "# # Assigning numerical values and storing in another column\n",
    "# shuff['label_cat'] = labelencoder.fit_transform(shuff['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "947b5cae-cbae-4af5-9d8c-7e048b7b5f77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>subjareas</th>\n",
       "      <th>subjareas_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Models that estimate land-use impacts on biodi...</td>\n",
       "      <td>['AGRI', 'ENVI']</td>\n",
       "      <td>[AGRI, ENVI]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This paper investigates food waste dynamics in...</td>\n",
       "      <td>['ECON', 'ENVI']</td>\n",
       "      <td>[ECON, ENVI]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Piwi-interacting RNAs (piRNAs) are important f...</td>\n",
       "      <td>['BIOC']</td>\n",
       "      <td>[BIOC]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The demise of Miocene carbonate build-ups in t...</td>\n",
       "      <td>['EART']</td>\n",
       "      <td>[EART]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Over recent decades, Land Use and Cover Change...</td>\n",
       "      <td>['ENVI']</td>\n",
       "      <td>[ENVI]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39520</th>\n",
       "      <td>Background To date, there have been few studie...</td>\n",
       "      <td>['MEDI', 'PSYC']</td>\n",
       "      <td>[MEDI, PSYC]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39521</th>\n",
       "      <td>We argue that a hexagonal grid with simple int...</td>\n",
       "      <td>['COMP', 'MATH']</td>\n",
       "      <td>[COMP, MATH]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39522</th>\n",
       "      <td>Experimental and theoretical studies show that...</td>\n",
       "      <td>['AGRI']</td>\n",
       "      <td>[AGRI]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39523</th>\n",
       "      <td>This review discusses recent developments in t...</td>\n",
       "      <td>['BIOC', 'PHAR']</td>\n",
       "      <td>[BIOC, PHAR]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39524</th>\n",
       "      <td>A system for the radiological protection of th...</td>\n",
       "      <td>['ENVI']</td>\n",
       "      <td>[ENVI]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39525 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                abstract         subjareas  \\\n",
       "0      Models that estimate land-use impacts on biodi...  ['AGRI', 'ENVI']   \n",
       "1      This paper investigates food waste dynamics in...  ['ECON', 'ENVI']   \n",
       "2      Piwi-interacting RNAs (piRNAs) are important f...          ['BIOC']   \n",
       "3      The demise of Miocene carbonate build-ups in t...          ['EART']   \n",
       "4      Over recent decades, Land Use and Cover Change...          ['ENVI']   \n",
       "...                                                  ...               ...   \n",
       "39520  Background To date, there have been few studie...  ['MEDI', 'PSYC']   \n",
       "39521  We argue that a hexagonal grid with simple int...  ['COMP', 'MATH']   \n",
       "39522  Experimental and theoretical studies show that...          ['AGRI']   \n",
       "39523  This review discusses recent developments in t...  ['BIOC', 'PHAR']   \n",
       "39524  A system for the radiological protection of th...          ['ENVI']   \n",
       "\n",
       "      subjareas_list  \n",
       "0       [AGRI, ENVI]  \n",
       "1       [ECON, ENVI]  \n",
       "2             [BIOC]  \n",
       "3             [EART]  \n",
       "4             [ENVI]  \n",
       "...              ...  \n",
       "39520   [MEDI, PSYC]  \n",
       "39521   [COMP, MATH]  \n",
       "39522         [AGRI]  \n",
       "39523   [BIOC, PHAR]  \n",
       "39524         [ENVI]  \n",
       "\n",
       "[39525 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# c08 - \n",
    "# we want to change \n",
    "# wrangling subjareas \n",
    "# change it to list from str\n",
    "\n",
    "def reg_ex(s:str) :\n",
    "    # input : str  - \"['MULT', 'SOCI']\"\n",
    "    # output: list - ['MULT', 'SOCI']\n",
    "    \n",
    "    inp = re.sub(\"[^a-zA-Z,]+\", \"\", s)\n",
    "    inp = inp.split(\",\")\n",
    "    return inp\n",
    "\n",
    "shuff['subjareas_list'] = list(map(reg_ex, shuff['subjareas'].tolist()))\n",
    "shuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8755d23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['AGRI', 'ARTS', 'BIOC', 'BUSI', 'CENG', 'CHEM', 'COMP', 'DECI',\n",
       "       'DENT', 'EART', 'ECON', 'ENER', 'ENGI', 'ENVI', 'HEAL', 'IMMU',\n",
       "       'MATE', 'MATH', 'MEDI', 'MULT', 'NEUR', 'NURS', 'PHAR', 'PHYS',\n",
       "       'PSYC', 'SOCI', 'VETE'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for sanity\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(shuff['subjareas_list'])\n",
    "\n",
    "mlb.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1eeae2b-8c68-4fee-adec-158d1ad1ccba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "{0: 'AGRI', 1: 'ARTS', 2: 'BIOC', 3: 'BUSI', 4: 'CENG', 5: 'CHEM', 6: 'COMP', 7: 'DECI', 8: 'DENT', 9: 'EART', 10: 'ECON', 11: 'ENER', 12: 'ENGI', 13: 'ENVI', 14: 'HEAL', 15: 'IMMU', 16: 'MATE', 17: 'MATH', 18: 'MEDI', 19: 'MULT', 20: 'NEUR', 21: 'NURS', 22: 'PHAR', 23: 'PHYS', 24: 'PSYC', 25: 'SOCI', 26: 'VETE'}\n"
     ]
    }
   ],
   "source": [
    "# for sanity\n",
    "class_name_list = list(mlb.classes_)\n",
    "class_dict = {}\n",
    "class_num = 0\n",
    "\n",
    "for class_name in class_name_list :\n",
    "    lbl_num = np.argmax(mlb.transform([[class_name]])[0])\n",
    "    class_dict[lbl_num] = class_name\n",
    "    class_num += 1\n",
    "    \n",
    "print(class_num)\n",
    "print(class_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c97dfa0-b221-43e7-b562-77bbc40b1912",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>subjareas</th>\n",
       "      <th>subjareas_list</th>\n",
       "      <th>label_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Models that estimate land-use impacts on biodi...</td>\n",
       "      <td>['AGRI', 'ENVI']</td>\n",
       "      <td>[AGRI, ENVI]</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This paper investigates food waste dynamics in...</td>\n",
       "      <td>['ECON', 'ENVI']</td>\n",
       "      <td>[ECON, ENVI]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Piwi-interacting RNAs (piRNAs) are important f...</td>\n",
       "      <td>['BIOC']</td>\n",
       "      <td>[BIOC]</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The demise of Miocene carbonate build-ups in t...</td>\n",
       "      <td>['EART']</td>\n",
       "      <td>[EART]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Over recent decades, Land Use and Cover Change...</td>\n",
       "      <td>['ENVI']</td>\n",
       "      <td>[ENVI]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39520</th>\n",
       "      <td>Background To date, there have been few studie...</td>\n",
       "      <td>['MEDI', 'PSYC']</td>\n",
       "      <td>[MEDI, PSYC]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39521</th>\n",
       "      <td>We argue that a hexagonal grid with simple int...</td>\n",
       "      <td>['COMP', 'MATH']</td>\n",
       "      <td>[COMP, MATH]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39522</th>\n",
       "      <td>Experimental and theoretical studies show that...</td>\n",
       "      <td>['AGRI']</td>\n",
       "      <td>[AGRI]</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39523</th>\n",
       "      <td>This review discusses recent developments in t...</td>\n",
       "      <td>['BIOC', 'PHAR']</td>\n",
       "      <td>[BIOC, PHAR]</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39524</th>\n",
       "      <td>A system for the radiological protection of th...</td>\n",
       "      <td>['ENVI']</td>\n",
       "      <td>[ENVI]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39525 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                abstract         subjareas  \\\n",
       "0      Models that estimate land-use impacts on biodi...  ['AGRI', 'ENVI']   \n",
       "1      This paper investigates food waste dynamics in...  ['ECON', 'ENVI']   \n",
       "2      Piwi-interacting RNAs (piRNAs) are important f...          ['BIOC']   \n",
       "3      The demise of Miocene carbonate build-ups in t...          ['EART']   \n",
       "4      Over recent decades, Land Use and Cover Change...          ['ENVI']   \n",
       "...                                                  ...               ...   \n",
       "39520  Background To date, there have been few studie...  ['MEDI', 'PSYC']   \n",
       "39521  We argue that a hexagonal grid with simple int...  ['COMP', 'MATH']   \n",
       "39522  Experimental and theoretical studies show that...          ['AGRI']   \n",
       "39523  This review discusses recent developments in t...  ['BIOC', 'PHAR']   \n",
       "39524  A system for the radiological protection of th...          ['ENVI']   \n",
       "\n",
       "      subjareas_list                                          label_cat  \n",
       "0       [AGRI, ENVI]  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...  \n",
       "1       [ECON, ENVI]  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, ...  \n",
       "2             [BIOC]  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3             [EART]  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...  \n",
       "4             [ENVI]  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...  \n",
       "...              ...                                                ...  \n",
       "39520   [MEDI, PSYC]  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "39521   [COMP, MATH]  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "39522         [AGRI]  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "39523   [BIOC, PHAR]  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "39524         [ENVI]  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...  \n",
       "\n",
       "[39525 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reff CELLNUM  : 11\n",
    "#      CELLNAME : currie\n",
    "# c11 - making label_cat to dataframe\n",
    "\n",
    "# desc: a workaround just like above c10 for sanity\n",
    "#       its just a workaround\n",
    "def mlb_transform(_list) :\n",
    "    return mlb.transform([_list])[0]\n",
    "\n",
    "shuff['label_cat'] = list(map(mlb_transform, shuff['subjareas_list']))\n",
    "shuff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a7f1a0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb544d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def custom_standardization(input_data, verbose= False):\n",
    "    # print original string\n",
    "    if verbose :\n",
    "        print(\"original string: \" ,input_data)\n",
    "    \n",
    "    # lowering\n",
    "    lowercase = input_data.lower()\n",
    "    \n",
    "    # print lowercase string\n",
    "    if verbose :\n",
    "        print('lower string: ', lowercase)\n",
    "    \n",
    "    \n",
    "    # clean the text\n",
    "    stripped = re.sub(\" #39;\", \"\\'\", lowercase)\n",
    "    stripped = re.sub(\" quot;\", \"\\\"\", stripped)\n",
    "    # remove stopword\n",
    "    # regex magic : r'\\b(' + r'|'.join(stopwords.words('english')) + r')\\b\\s*'\n",
    "    stripped = re.sub(r'\\b(' + r'|'.join(stopwords.words('english')) + r')\\b\\s*',\"\", stripped)\n",
    "    \n",
    "    # print remove stopword string\n",
    "    if verbose :\n",
    "        print('Stopword removal: ', stripped)\n",
    "    \n",
    "    # strip punctuiation\n",
    "    stripped = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", stripped)\n",
    "    # remove double space\n",
    "    # two or more whitespace\n",
    "    stripped = re.sub(r\"(\\s\\s*)\", \" \", stripped)\n",
    "    stripped = re.sub(r\"(\\s+$)\", \"\", stripped)\n",
    "    \n",
    "    # tokenize\n",
    "    tokens = word_tokenize(stripped)\n",
    "\n",
    "    # print tokenize\n",
    "    if verbose :\n",
    "        print(\"toknize: \", tokens)\n",
    "    \n",
    "    # stemming\n",
    "    temp_cont = [porterStemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    # print stemmed\n",
    "    if verbose:\n",
    "        print(\"stemmed: \", temp_cont)\n",
    "    \n",
    "    porter_    = ' '.join(temp_cont)\n",
    "    return porter_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa64e1f5-46cd-452e-8b1b-4681a7483f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Testing\n",
    "# t_list = [\"The boys thought it was quite remarkable that the oxen never turned north throughout the days\",\n",
    "#          \"The fir tree has a comforting smell as it sways in the wind\",\n",
    "#          \"My house is ablaze because I forgot to turn off the oven\",\n",
    "#          \"Grandma loves soft bread and tea, though she hates eating meat given her age\",\n",
    "#          \"The three-legged goat running across the irradiated field\"]\n",
    "# # t_list\n",
    "# # for report\n",
    "# # t_list = copy.deepcopy(shuff['abstract'].tolist()[1000:1005])\n",
    "# for t in t_list :\n",
    "#     tp = custom_standardization(t, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f16cd9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cell:map_cleaning-text \n",
      "Execution Time : 211.26 s\n",
      "CPU times: total: 3min 31s\n",
      "Wall time: 3min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# CELLNUM : 13\n",
    "# CELLNAME: clown\n",
    "\n",
    "c13_start = datetime.now()\n",
    "# clean text data\n",
    "clean_text = list(map(custom_standardization, shuff['abstract'].tolist()))\n",
    "shuff['clean_text'] = clean_text\n",
    "\n",
    "c13_duration = datetime.now() - c13_start\n",
    "print(\"cell:map_cleaning-text \\nExecution Time : {:.2f} s\".format(c13_duration.total_seconds()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cbbb9fdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>subjareas</th>\n",
       "      <th>subjareas_list</th>\n",
       "      <th>label_cat</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Models that estimate land-use impacts on biodi...</td>\n",
       "      <td>['AGRI', 'ENVI']</td>\n",
       "      <td>[AGRI, ENVI]</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "      <td>model estim land use impact biodivers multipl ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This paper investigates food waste dynamics in...</td>\n",
       "      <td>['ECON', 'ENVI']</td>\n",
       "      <td>[ECON, ENVI]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, ...</td>\n",
       "      <td>paper investig food wast dynam retail altern f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Piwi-interacting RNAs (piRNAs) are important f...</td>\n",
       "      <td>['BIOC']</td>\n",
       "      <td>[BIOC]</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>piwi interact rna pirna import genom regul acr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The demise of Miocene carbonate build-ups in t...</td>\n",
       "      <td>['EART']</td>\n",
       "      <td>[EART]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>demis miocen carbon build up brows basin north...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Over recent decades, Land Use and Cover Change...</td>\n",
       "      <td>['ENVI']</td>\n",
       "      <td>[ENVI]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "      <td>recent decad land use cover chang lucc trend m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39520</th>\n",
       "      <td>Background To date, there have been few studie...</td>\n",
       "      <td>['MEDI', 'PSYC']</td>\n",
       "      <td>[MEDI, PSYC]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>background date studi dmdd examin risk factor ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39521</th>\n",
       "      <td>We argue that a hexagonal grid with simple int...</td>\n",
       "      <td>['COMP', 'MATH']</td>\n",
       "      <td>[COMP, MATH]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>argu hexagon grid simpl intermedi node robust ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39522</th>\n",
       "      <td>Experimental and theoretical studies show that...</td>\n",
       "      <td>['AGRI']</td>\n",
       "      <td>[AGRI]</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>experiment theoret studi show mortal impos pop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39523</th>\n",
       "      <td>This review discusses recent developments in t...</td>\n",
       "      <td>['BIOC', 'PHAR']</td>\n",
       "      <td>[BIOC, PHAR]</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>review discuss recent develop use non code rna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39524</th>\n",
       "      <td>A system for the radiological protection of th...</td>\n",
       "      <td>['ENVI']</td>\n",
       "      <td>[ENVI]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "      <td>system radiolog protect environ wildlif base r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39525 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                abstract         subjareas  \\\n",
       "0      Models that estimate land-use impacts on biodi...  ['AGRI', 'ENVI']   \n",
       "1      This paper investigates food waste dynamics in...  ['ECON', 'ENVI']   \n",
       "2      Piwi-interacting RNAs (piRNAs) are important f...          ['BIOC']   \n",
       "3      The demise of Miocene carbonate build-ups in t...          ['EART']   \n",
       "4      Over recent decades, Land Use and Cover Change...          ['ENVI']   \n",
       "...                                                  ...               ...   \n",
       "39520  Background To date, there have been few studie...  ['MEDI', 'PSYC']   \n",
       "39521  We argue that a hexagonal grid with simple int...  ['COMP', 'MATH']   \n",
       "39522  Experimental and theoretical studies show that...          ['AGRI']   \n",
       "39523  This review discusses recent developments in t...  ['BIOC', 'PHAR']   \n",
       "39524  A system for the radiological protection of th...          ['ENVI']   \n",
       "\n",
       "      subjareas_list                                          label_cat  \\\n",
       "0       [AGRI, ENVI]  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...   \n",
       "1       [ECON, ENVI]  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, ...   \n",
       "2             [BIOC]  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3             [EART]  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...   \n",
       "4             [ENVI]  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...   \n",
       "...              ...                                                ...   \n",
       "39520   [MEDI, PSYC]  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39521   [COMP, MATH]  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39522         [AGRI]  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39523   [BIOC, PHAR]  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39524         [ENVI]  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...   \n",
       "\n",
       "                                              clean_text  \n",
       "0      model estim land use impact biodivers multipl ...  \n",
       "1      paper investig food wast dynam retail altern f...  \n",
       "2      piwi interact rna pirna import genom regul acr...  \n",
       "3      demis miocen carbon build up brows basin north...  \n",
       "4      recent decad land use cover chang lucc trend m...  \n",
       "...                                                  ...  \n",
       "39520  background date studi dmdd examin risk factor ...  \n",
       "39521  argu hexagon grid simpl intermedi node robust ...  \n",
       "39522  experiment theoret studi show mortal impos pop...  \n",
       "39523  review discuss recent develop use non code rna...  \n",
       "39524  system radiolog protect environ wildlif base r...  \n",
       "\n",
       "[39525 rows x 5 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b17a9b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Build TF-IDF SKlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8ce21d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector Size :  1784\n",
      "cell:train_tf-idf \n",
      "Execution Time : 13.46 s\n",
      "CPU times: total: 13.4 s\n",
      "Wall time: 13.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# CELLNUM  : 15\n",
    "# CELLNAME : zoan\n",
    "\n",
    "c15_start = datetime.now()\n",
    "# generate & fit\n",
    "vectorizer = TfidfVectorizer(min_df=0.01, \n",
    "                             max_df = 0.8, \n",
    "                             ngram_range=(1,2))\n",
    "vectorizer.fit(shuff['clean_text'])\n",
    "dim_size = vectorizer.get_feature_names_out().shape[0]\n",
    "print(\"Vector Size : \", dim_size)\n",
    "c15_duration = datetime.now() - c15_start\n",
    "print(\"cell:train_tf-idf \\nExecution Time : {0:.2f} s\".format(c15_duration.total_seconds()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31b615a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## K-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7219f159",
   "metadata": {},
   "outputs": [],
   "source": [
    "K5_fold = KFold(n_splits=5)\n",
    "fold = []\n",
    "# fold -> [[train_idex, val_index, test_index], ...,[train_idex, val_index, test_index]]\n",
    "\n",
    "for train_index, test_index in K5_fold.split(shuff['clean_text']):\n",
    "    val_index = test_index[int(len(test_index) / 2): ]\n",
    "    test_index = test_index[:int(len(test_index)/2)]\n",
    "    fold.append([train_index, val_index, test_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa6ca05-dfba-4fe9-b614-3e621c727b1d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58391b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Build NN\n",
    "Build a NN from predetermined composition <br>\n",
    "Need `input_size` as a parameter          <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "247df081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "def build_nn(input_size:int):\n",
    "    input_layer = layers.Input(shape=(input_size,))\n",
    "\n",
    "    hdn  = layers.Dense(900, activation='selu') (input_layer)\n",
    "    hdn = layers.BatchNormalization(momentum=0.99, epsilon=0.0001)(hdn)\n",
    "    hdn  = layers.Dense(600, activation='elu') (hdn)\n",
    "    hdn = layers.BatchNormalization(momentum=0.99, epsilon=0.0001)(hdn)\n",
    "    hdn  = layers.Dense(900, activation='selu') (hdn)\n",
    "    hdn  = layers.Dense(700, activation='selu') (hdn)\n",
    "\n",
    "    # using logits so this is how the output look\n",
    "    predictions = layers.Dense(27, activation='sigmoid', name=\"predictions\")(hdn)\n",
    "    \n",
    "    model = tf.keras.Model(input_layer, predictions)\n",
    "    loss=tf.keras.losses.BinaryCrossentropy()\n",
    "    # Compile the model with binary crossentropy loss and an adam optimizer.\n",
    "    model.compile(loss= loss,\n",
    "              optimizer=\"adam\", \n",
    "              metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05994541-d6e2-4719-a0f9-62a8c84ec46b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Metrics for ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39cb1c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to classification_report homebrew\n",
    "def _report_classification(confusion_matrix, class_name= class_dict, as_df=True) :\n",
    "    # class_name -> a dict with int as key and str as value\n",
    "    \n",
    "    kf_tbl = {}\n",
    "    for i in range(confusion_matrix.shape[0]) :\n",
    "        c_name = class_name[i]\n",
    "        elm_conf_matrix = confusion_matrix[i]\n",
    "        kf_tbl[c_name] = {}\n",
    "        # TP \n",
    "        kf_tbl[c_name][\"TP\"] = elm_conf_matrix[1,1]\n",
    "        # FN\n",
    "        # baris koma belakang\n",
    "        kf_tbl[c_name][\"FN\"] = elm_conf_matrix[1,0]\n",
    "        # FP\n",
    "        # klo liat kolom koma depan\n",
    "        kf_tbl[c_name][\"FP\"] = elm_conf_matrix[0,1]\n",
    "        # FN\n",
    "        kf_tbl[c_name][\"TN\"] = elm_conf_matrix[0,0]\n",
    "        # Precision\n",
    "        kf_tbl[c_name][\"Precision\"] = np.nan_to_num(kf_tbl[c_name][\"TP\"]/(kf_tbl[c_name][\"TP\"] + kf_tbl[c_name][\"FP\"]))\n",
    "\n",
    "        # Recall\n",
    "        kf_tbl[c_name][\"Recall\"] = np.nan_to_num((kf_tbl[c_name][\"TP\"]/(kf_tbl[c_name][\"TP\"] + kf_tbl[c_name][\"FN\"])))\n",
    "        # f1-score per-class\n",
    "        _temp = (kf_tbl[c_name][\"Precision\"]* kf_tbl[c_name][\"Recall\"])\n",
    "        _temp = _temp/(kf_tbl[c_name][\"Precision\"] + kf_tbl[c_name][\"Recall\"])\n",
    "        _temp = np.nan_to_num(_temp)\n",
    "        kf_tbl[c_name][\"f1-score\"] = np.nan_to_num(2*_temp)\n",
    "        \n",
    "    if as_df : \n",
    "        t_tbl = {\"labels\" : [],\n",
    "                 \"TP\"     : [],\n",
    "                 \"FN\"     : [],\n",
    "                 \"FP\"     : [],\n",
    "                 \"TN\"     : [],\n",
    "                 \"Precision\" : [],\n",
    "                 \"Recall\"    : [],\n",
    "                 \"f1-score\"  : []\n",
    "                }\n",
    "        for key in list(kf_tbl.keys()) :\n",
    "            t_tbl['labels'].append(key)\n",
    "            t_tbl['TP'].append(kf_tbl[key]['TP'])\n",
    "            t_tbl['FN'].append(kf_tbl[key]['FN'])\n",
    "            t_tbl['FP'].append(kf_tbl[key]['FP'])\n",
    "            t_tbl['TN'].append(kf_tbl[key]['TN'])\n",
    "            t_tbl['Precision'].append(kf_tbl[key]['Precision'])\n",
    "            t_tbl['Recall'].append(kf_tbl[key]['Recall'])\n",
    "            t_tbl['f1-score'].append(kf_tbl[key]['f1-score'])\n",
    "        df = pd.DataFrame.from_dict(t_tbl)\n",
    "        df = df.set_index('labels')\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    return kf_tbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19ee15b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_f1(cls_report) :\n",
    "    cls_report_type = type(cls_report)\n",
    "    if cls_report_type is dict :\n",
    "        cls_num = len(list(cls_report.keys()))\n",
    "        cls_keys = list(cls_report.keys())\n",
    "        \n",
    "        # macro\n",
    "        # get all f1-score then just average it to cls_num\n",
    "        _f1_list = []\n",
    "        for key in cls_keys :\n",
    "            _f1_list.append(cls_report[key]['f1-score'])\n",
    "        _macro_f1 = sum(_f1_list) / cls_num\n",
    "        _f1_list = None\n",
    "        \n",
    "        # weigthed\n",
    "        # support_proportion = support / sum(support)\n",
    "        # sum(f1*support_proportion)\n",
    "        _f1_list = []\n",
    "        _sum_support = 0\n",
    "        for key in cls_keys :\n",
    "            _support = cls_report[key]['TP'] + cls_report[key]['FN']\n",
    "            _f1_list.append(cls_report[key]['f1-score'] * _support)\n",
    "            _sum_support += _support\n",
    "        _weighted_f1 = sum(_f1_list) / _sum_support\n",
    "        _f1_list = None\n",
    "                        \n",
    "        # micro\n",
    "        # all_class_TP FP  amnd FN\n",
    "        # using that just count f1 normally\n",
    "        _sum_TP = 0\n",
    "        _sum_FP = 0\n",
    "        _sum_FN = 0\n",
    "        for key in cls_keys :\n",
    "            _sum_TP += cls_report[key]['TP']\n",
    "            _sum_FP += cls_report[key]['FP']\n",
    "            _sum_FN += cls_report[key]['FN']\n",
    "        _micro_f1 = _sum_TP / (_sum_TP + (0.5*(_sum_FP + _sum_FN)))\n",
    "                        \n",
    "        rslt = {'f1' : {\n",
    "                'micro'    : _micro_f1,\n",
    "                'macro'    : _macro_f1,\n",
    "                'weighted' : _weighted_f1}\n",
    "              }\n",
    "        return rslt\n",
    "    elif cls_report_type is pd.core.frame.DataFrame :\n",
    "        cls_num = cls_report.shape[0]\n",
    "        \n",
    "        # macro\n",
    "        _macro_f1 = sum(cls_report['f1-score'].tolist()) / len(cls_report['f1-score'].tolist())\n",
    "        \n",
    "        # weighted\n",
    "        # support = tp + fn\n",
    "        _cls_TP = cls_report['TP'].tolist()\n",
    "        _cls_FN = cls_report['FN'].tolist()\n",
    "        _cls_f1 = cls_report['f1-score'].tolist()\n",
    "        \n",
    "        _support = [_cls_TP[i] + _cls_FN[i] for i in range(len(_cls_TP))]\n",
    "        _sum_support = sum(_support)\n",
    "        \n",
    "        # sum(f1[i] * (support[i] / sum(support)))\n",
    "        _weighted_f1 = sum([_cls_f1[i] * (_support[i] / _sum_support) for i in range(len(_cls_f1))])\n",
    "        \n",
    "        # micro\n",
    "        _sum_TP = sum(cls_report['TP'].tolist())\n",
    "        _sum_FP = sum(cls_report['FP'].tolist())\n",
    "        _sum_FN = sum(cls_report['FN'].tolist())\n",
    "        _micro_f1 = _sum_TP / (_sum_TP + (0.5*(_sum_FP + _sum_FN)))\n",
    "        \n",
    "        rslt = {'f1' : {\n",
    "                'micro'    : _micro_f1,\n",
    "                'macro'    : _macro_f1,\n",
    "                'weighted' : _weighted_f1}\n",
    "              }\n",
    "        return rslt\n",
    "    else :\n",
    "        err = {'f1' : {\n",
    "                'micro'    : -4473,\n",
    "                'macro'    : -4473,\n",
    "                'weighted' : -4473}\n",
    "              }\n",
    "        \n",
    "        return err\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05c499e9-2467-44d2-9b23-724291b09364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOCAL CONSTANT\n",
    "# single most important\n",
    "\n",
    "THRESHOLD = 0.5\n",
    "\n",
    "def _to_multi_hot(float_pred) :\n",
    "    # use map\n",
    "    _pred_bool = float_pred > THRESHOLD\n",
    "    return _pred_bool.astype(int)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ab59c97-55ca-45d6-a819-0d13667f3077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp save protocol\n",
    "# save model to file \n",
    "# purge session \n",
    "# reload the model\n",
    "\n",
    "def _blaze_p(ae, encoder) :\n",
    "    t_ae_name = ae._name\n",
    "    t_e_name = encoder._name\n",
    "    \n",
    "    f_ae_name = \"{}.h5\".format(t_ae_name)\n",
    "    f_e_name = \"{}.h5\".format(t_e_name)\n",
    "    \n",
    "    ae.save(f_ae_name)\n",
    "    encoder.save(f_e_name)\n",
    "    \n",
    "    \n",
    "    # clear session, rem from memory\n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    ae = models.load_model(f_ae_name)\n",
    "    encoder = models.load_model(f_e_name)\n",
    "    \n",
    "    \n",
    "    return ae, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25866b55-9fdb-4a9b-97b1-836a530ebb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _average_dataframe(list_dframe: list) :\n",
    "    for i in range(len(list_dframe)) :\n",
    "        if i == 0 :\n",
    "            f_avg = list_dframe[i]\n",
    "        else :\n",
    "            f_avg = f_avg.add(list_dframe[i])\n",
    "\n",
    "    # divide by amount of fold\n",
    "    f_avg = f_avg.div(len(list_dframe))\n",
    "\n",
    "    # round down\n",
    "    f_avg['TP'] = f_avg['TP'].astype('int')\n",
    "    f_avg['FN'] = f_avg['FN'].astype('int')\n",
    "    f_avg['FP'] = f_avg['FP'].astype('int')\n",
    "    f_avg['TN'] = f_avg['TN'].astype('int')\n",
    "    \n",
    "    # return an average of all dataframe\n",
    "    return f_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f98966-e7ee-4cfb-bd59-3a68cddd74a6",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "Evaluate the f1-measure and acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9614d76a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Function to Evaluate Vanilla Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cf9b7849-9fe1-49f8-a7ee-66f20f0751ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _eval_control_multi(save_file=False) :\n",
    "    # i  - counter\n",
    "    i = 1\n",
    "    \n",
    "    # t_res -> f1-macro foreach fold\n",
    "    # t_loss-> loss foreach fold\n",
    "    # t_acc -> acc foreach fold\n",
    "    t_res = []\n",
    "    t_loss = []\n",
    "    t_acc =[]\n",
    "    \n",
    "    t_hamming_loss = []\n",
    "    t_acc_score = []\n",
    "    list_dataframe = []\n",
    "    for train_index, val_index, test_index in fold :\n",
    "        mf_cstart = datetime.now()\n",
    "        \n",
    "        # clear session _del model from memory\n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "        # prepare dataset\n",
    "        X_train = vectorizer.transform(shuff['clean_text'][train_index]).toarray()\n",
    "        X_val = vectorizer.transform(shuff['clean_text'][val_index]).toarray()\n",
    "        X_test = vectorizer.transform(shuff['clean_text'][test_index]).toarray()\n",
    "        \n",
    "        \n",
    "        y_train = np.array(shuff['label_cat'][train_index].tolist()).ravel()\n",
    "        y_train = np.reshape(y_train, (int(y_train.shape[0]/class_num), class_num))\n",
    "        \n",
    "        y_val = np.array(shuff['label_cat'][val_index].tolist()).ravel()\n",
    "        y_val = np.reshape(y_val, (int(y_val.shape[0]/class_num), class_num))\n",
    "\n",
    "        y_test = np.array(shuff['label_cat'][test_index].tolist()).ravel()\n",
    "        y_test = np.reshape(y_test, (int(y_test.shape[0]/class_num), class_num))\n",
    "        \n",
    "        \n",
    "        # build nn\n",
    "        nn_name = \"multi_control_{}\".format(i)\n",
    "        nn_model = build_nn(input_size=dim_size)\n",
    "        nn_model._name = nn_name\n",
    "        # print(nn_model._name)\n",
    "        \n",
    "        # train - fit\n",
    "        fit_history = nn_model.fit(X_train, \n",
    "                                     y_train, \n",
    "                                     epochs=100, \n",
    "                                     batch_size=75, \n",
    "                                     validation_data = (X_val, y_val),\n",
    "                                     verbose=0)\n",
    "        \n",
    "\n",
    "        # test\n",
    "        # save confusion matrix too\n",
    "        print(\"Evaluate on test data \", i)\n",
    "        results = nn_model.evaluate(X_test, y_test, batch_size=128, verbose=0)\n",
    "        print(\"test loss, test acc:\", results)\n",
    "        pred = nn_model.predict(X_test, verbose=0)\n",
    "        pred = list(map(_to_multi_hot, pred))\n",
    "        \n",
    "\n",
    "        result_df = _report_classification(multilabel_confusion_matrix(y_test, pred))\n",
    "        result_metric = get_f1(result_df)\n",
    "        \n",
    "        \n",
    "        list_dataframe.append(result_df)\n",
    "        t_res.append(result_metric['f1']['macro'])\n",
    "        t_loss.append(results[0])\n",
    "        t_acc.append(results[1])\n",
    "        # get hamming loss and accuracy score\n",
    "        t_hamming_loss.append(hamming_loss(y_test, pred))\n",
    "        t_acc_score.append(accuracy_score(y_test, pred))\n",
    "        print('hamming loss:' ,t_hamming_loss[-1])\n",
    "        print('accuracy score:' ,t_acc_score[-1])\n",
    "        print('f1-macro:', t_res[-1])\n",
    "        mf_cdur = datetime.now() - mf_cstart\n",
    "        print(\"func:eval_model on fold {} \\nExecution Time : {:.2f} s\".format(i , mf_cdur.total_seconds()))\n",
    "        print()\n",
    "        \n",
    "        \n",
    "        \n",
    "        i += 1\n",
    "    \n",
    "    d_result = {'f1-macro' : sum(t_res) / len(t_res),\n",
    "                'loss' : sum(t_loss) / len(t_loss),\n",
    "                'bin_acc'  : sum(t_acc) / len(t_acc), \n",
    "                'hamming_loss' : sum(t_hamming_loss) / len(t_hamming_loss),\n",
    "                'accuracy_score' : sum(t_acc_score) / len(t_acc_score),\n",
    "                'avg_conf' : _average_dataframe(list_dataframe)\n",
    "               }\n",
    "    \n",
    "    return d_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc35618-7b95-4563-a4d1-bf87bc92df61",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Control Experiment Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d451a378-2393-40a4-a577-2ddab746fd2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test data  1\n",
      "test loss, test acc: [0.8149200081825256, 0.9445756673812866]\n",
      "hamming loss: 0.055424351476983054\n",
      "accuracy score: 0.2866902834008097\n",
      "f1-macro: 0.48424994526696785\n",
      "func:eval_model on fold 1 \n",
      "Execution Time : 430.58 s\n",
      "\n",
      "Evaluate on test data  2\n",
      "test loss, test acc: [0.7216058969497681, 0.9443038702011108]\n",
      "hamming loss: 0.05569613135402609\n",
      "accuracy score: 0.2932692307692308\n",
      "f1-macro: 0.47169829973937555\n",
      "func:eval_model on fold 2 \n",
      "Execution Time : 416.27 s\n",
      "\n",
      "Evaluate on test data  3\n",
      "test loss, test acc: [0.7562237977981567, 0.9433104395866394]\n",
      "hamming loss: 0.05668953366321788\n",
      "accuracy score: 0.2909919028340081\n",
      "f1-macro: 0.47495172576999484\n",
      "func:eval_model on fold 3 \n",
      "Execution Time : 419.82 s\n",
      "\n",
      "Evaluate on test data  4\n",
      "test loss, test acc: [0.7983012795448303, 0.9448005557060242]\n",
      "hamming loss: 0.0551994301994302\n",
      "accuracy score: 0.28846153846153844\n",
      "f1-macro: 0.4821293447299008\n",
      "func:eval_model on fold 4 \n",
      "Execution Time : 421.16 s\n",
      "\n",
      "Evaluate on test data  5\n",
      "test loss, test acc: [0.7627577781677246, 0.943760335445404]\n",
      "hamming loss: 0.05623969110811216\n",
      "accuracy score: 0.2889676113360324\n",
      "f1-macro: 0.4683253313448972\n",
      "func:eval_model on fold 5 \n",
      "Execution Time : 432.47 s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "control_measurement = _eval_control_multi(save_file=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e6b8429a-1176-41ff-8eb0-b5154ffbcd43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1-macro': 0.47627092937022725, 'loss': 0.7707617521286011, 'bin_acc': 0.944150173664093, 'hamming_loss': 0.05584982756035388, 'accuracy_score': 0.2896761133603239, 'avg_conf':          TP   FN   FP    TN  Precision    Recall  f1-score\n",
      "labels                                                    \n",
      "AGRI    194  212  139  3406   0.590075  0.477668  0.524926\n",
      "ARTS     25   74   31  3820   0.440246  0.248895  0.314058\n",
      "BIOC    492  335  230  2894   0.684136  0.594202  0.633876\n",
      "BUSI     31   64   49  3807   0.397421  0.325931  0.353876\n",
      "CENG     66  141   73  3670   0.474820  0.322316  0.382505\n",
      "CHEM    106  161  111  3573   0.487187  0.396107  0.434863\n",
      "COMP    121  171  107  3551   0.540172  0.414569  0.463915\n",
      "DECI      9   45   22  3875   0.304303  0.166893  0.212175\n",
      "DENT      0    4    0  3947   0.000000  0.000000  0.000000\n",
      "EART    175  102   79  3594   0.688861  0.631721  0.659036\n",
      "ECON     38   72   24  3816   0.631852  0.345094  0.439079\n",
      "ENER    150  133   98  3570   0.604917  0.532089  0.564381\n",
      "ENGI    287  303  195  3166   0.600081  0.487931  0.533296\n",
      "ENVI    400  225  273  3052   0.596848  0.639388  0.615672\n",
      "HEAL     12   67   21  3851   0.417256  0.155679  0.215874\n",
      "IMMU    184  133   88  3546   0.685992  0.580679  0.624890\n",
      "MATE    234  167   90  3459   0.723404  0.582183  0.643921\n",
      "MATH     55  101   46  3748   0.545452  0.354422  0.428675\n",
      "MEDI    639  259  219  2833   0.746952  0.711365  0.727632\n",
      "MULT     11   15    9  3916   0.595877  0.424485  0.472370\n",
      "NEUR    191  135   68  3557   0.740530  0.586112  0.652692\n",
      "NURS      5   22   11  3912   0.389062  0.191721  0.245413\n",
      "PHAR     88  131   67  3665   0.570241  0.404289  0.470271\n",
      "PHYS    198  195  116  3442   0.631253  0.502272  0.558309\n",
      "PSYC     71   96   58  3725   0.550687  0.423572  0.473445\n",
      "SOCI    174  181  112  3484   0.609471  0.489783  0.541247\n",
      "VETE     61   41   19  3830   0.763879  0.602966  0.672917}\n",
      "AVERAGE PRECISION :  0.5559620580084508\n",
      "AVERAGE RECALL :  0.4293456077432577\n"
     ]
    }
   ],
   "source": [
    "print(control_measurement)\n",
    "print(\"AVERAGE PRECISION : \", control_measurement['avg_conf']['Precision'].mean())\n",
    "print(\"AVERAGE RECALL : \", control_measurement['avg_conf']['Recall'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6218a1c3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Function to Compare with other alg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b16762-3c63-4c10-b8af-dfaa0d407457",
   "metadata": {
    "tags": []
   },
   "source": [
    "### SVM Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b51e9e7a-a4d5-4d02-8837-f61df26b49df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _eval_svm(save_file=False) :\n",
    "    i = 1\n",
    "    t_res = []\n",
    "    list_dataframe = []\n",
    "    for train_index, val_index, test_index in fold :\n",
    "        # print(train_index, val_index, test_index)\n",
    "        \n",
    "        ## ERROE\n",
    "        # train_index = train_index[:5]\n",
    "        # val_index = val_index[:5]\n",
    "        # test_index = test_index[:5]\n",
    "        \n",
    "        \n",
    "        f_svmstart = datetime.now()\n",
    "        \n",
    "        \n",
    "        X_train = vectorizer.transform(shuff['clean_text'][train_index]).toarray()\n",
    "        X_val = vectorizer.transform(shuff['clean_text'][val_index]).toarray()\n",
    "        X_test = vectorizer.transform(shuff['clean_text'][test_index]).toarray()\n",
    "        \n",
    "        print(\"x train:\",X_train.shape)\n",
    "        print(\"x val  :\",X_val.shape)\n",
    "        print(\"x test :\",X_test.shape)\n",
    "        # print(type(X_train))\n",
    "        \n",
    "        # print(X_val)\n",
    "        # i dont know\n",
    "        y_train = np.array(shuff['label_cat'][train_index].tolist()).ravel()\n",
    "        y_train = np.reshape(y_train, (int(y_train.shape[0]/class_num), class_num))\n",
    "        \n",
    "        y_val = np.array(shuff['label_cat'][val_index].tolist()).ravel()\n",
    "        y_val = np.reshape(y_val, (int(y_val.shape[0]/class_num), class_num))\n",
    "\n",
    "        y_test = np.array(shuff['label_cat'][test_index].tolist()).ravel()\n",
    "        y_test = np.reshape(y_test, (int(y_test.shape[0]/class_num), class_num))\n",
    "        \n",
    "        print(\"y train:\",y_train.shape)\n",
    "        print(\"y val  :\",y_val.shape)\n",
    "        print(\"y test :\",y_test.shape)\n",
    "        \n",
    "        print(\"unique train:\", np.unique(y_train, axis=0).shape[0])\n",
    "        print(\"unique val  :\", np.unique(y_val, axis=0).shape[0])\n",
    "        print(\"unique test :\", np.unique(y_test, axis=0).shape[0])\n",
    "        \n",
    "        # print(y_val)\n",
    "        # print(y_train[:5])\n",
    "        \n",
    "        t_time = datetime.now()\n",
    "        print(\"prepare training fold-{} .... took a while\".format(i))\n",
    "        \n",
    "        \n",
    "        clf = MultiOutputClassifier(svm.SVC(kernel='linear', class_weight ='balanced',decision_function_shape='ovr')).fit(X_train, y_train)\n",
    "        t_dur = datetime.now() - t_time  \n",
    "        print('elapsed time for training : {:2f} s'.format(t_dur.total_seconds()))\n",
    "        print('fold-{} ...done training'.format(i))\n",
    "        pred = clf.predict(X_test)\n",
    "        \n",
    "        result_df = _report_classification(multilabel_confusion_matrix(y_test, pred))\n",
    "        \n",
    "        print(result_df)\n",
    "        print(get_f1(result_df))\n",
    "        list_dataframe.append(result_df)\n",
    "        # delete\n",
    "        X_train, X_val,X_test = None, None, None\n",
    "        y_train, y_val,y_test = None, None, None\n",
    "        clf = None\n",
    "        \n",
    "        print('finish fold-{}'.format(i))\n",
    "        \n",
    "        i += 1\n",
    "    # average all fold\n",
    "    result_avg = _average_dataframe(list_dataframe)\n",
    "    result_f1 = get_f1(result_avg)\n",
    "\n",
    "    f_svmduration = datetime.now() - f_svmstart\n",
    "    print(\"func:evaluate_svm \\nExecution Time : {:2f} s\".format(f_svmduration.total_seconds()))\n",
    "    return {'result' : result_avg,\n",
    "            'f1'     : result_f1}\n",
    "    \n",
    "# x = _eval_svm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "408780a9-de6e-4c04-aa7c-e228ef556484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x train: (31620, 1784)\n",
      "x val  : (3953, 1784)\n",
      "x test : (3952, 1784)\n",
      "y train: (31620, 27)\n",
      "y val  : (3953, 27)\n",
      "y test : (3952, 27)\n",
      "unique train: 332\n",
      "unique val  : 269\n",
      "unique test : 259\n",
      "prepare training fold-1 .... took a while\n",
      "elapsed time for training : 7730.191265 s\n",
      "fold-1 ...done training\n",
      "         TP   FN   FP    TN  Precision    Recall  f1-score\n",
      "labels                                                    \n",
      "AGRI    324  113  489  3026   0.398524  0.741419  0.518400\n",
      "ARTS     61   37  297  3557   0.170391  0.622449  0.267544\n",
      "BIOC    664  173  486  2629   0.577391  0.793309  0.668344\n",
      "BUSI     61   27  275  3589   0.181548  0.693182  0.287736\n",
      "CENG    161   38  573  3180   0.219346  0.809045  0.345123\n",
      "CHEM    245   56  563  3088   0.303218  0.813953  0.441839\n",
      "COMP    242   62  548  3100   0.306329  0.796053  0.442413\n",
      "DECI     21   26  152  3753   0.121387  0.446809  0.190909\n",
      "DENT      0    8    0  3944   0.000000  0.000000  0.000000\n",
      "EART    241   42  336  3333   0.417678  0.851590  0.560465\n",
      "ECON     88   23  236  3605   0.271605  0.792793  0.404598\n",
      "ENER    228   58  377  3289   0.376860  0.797203  0.511785\n",
      "ENGI    488   94  604  2766   0.446886  0.838488  0.583035\n",
      "ENVI    524  101  545  2782   0.490178  0.838400  0.618654\n",
      "HEAL     40   41  282  3589   0.124224  0.493827  0.198511\n",
      "IMMU    264   43  396  3249   0.400000  0.859935  0.546019\n",
      "MATE    333   64  383  3172   0.465084  0.838791  0.598383\n",
      "MATH    120   35  421  3376   0.221811  0.774194  0.344828\n",
      "MEDI    771  151  384  2646   0.667532  0.836226  0.742417\n",
      "MULT     11   11   34  3896   0.244444  0.500000  0.328358\n",
      "NEUR    290   36  310  3316   0.483333  0.889571  0.626350\n",
      "NURS     18   16   56  3862   0.243243  0.529412  0.333333\n",
      "PHAR    164   45  446  3297   0.268852  0.784689  0.400488\n",
      "PHYS    326   61  513  3052   0.388558  0.842377  0.531811\n",
      "PSYC    135   28  327  3462   0.292208  0.828221  0.432000\n",
      "SOCI    304   53  504  3091   0.376238  0.851541  0.521888\n",
      "VETE     90   24   76  3762   0.542169  0.789474  0.642857\n",
      "{'f1': {'micro': 0.5286935806355554, 'macro': 0.4477069712271647, 'weighted': 0.5546874478114363}}\n",
      "finish fold-1\n",
      "x train: (31620, 1784)\n",
      "x val  : (3953, 1784)\n",
      "x test : (3952, 1784)\n",
      "y train: (31620, 27)\n",
      "y val  : (3953, 27)\n",
      "y test : (3952, 27)\n",
      "unique train: 332\n",
      "unique val  : 264\n",
      "unique test : 260\n",
      "prepare training fold-2 .... took a while\n",
      "elapsed time for training : 7565.355383 s\n",
      "fold-2 ...done training\n",
      "         TP   FN   FP    TN  Precision    Recall  f1-score\n",
      "labels                                                    \n",
      "AGRI    304   71  517  3060   0.370280  0.810667  0.508361\n",
      "ARTS     50   38  237  3627   0.174216  0.568182  0.266667\n",
      "BIOC    638  167  513  2634   0.554301  0.792547  0.652352\n",
      "BUSI     58   29  262  3603   0.181250  0.666667  0.285012\n",
      "CENG    175   41  609  3127   0.223214  0.810185  0.350000\n",
      "CHEM    211   52  591  3098   0.263092  0.802281  0.396244\n",
      "COMP    195   70  512  3175   0.275813  0.735849  0.401235\n",
      "DECI     22   28  160  3742   0.120879  0.440000  0.189655\n",
      "DENT      0    3    1  3948   0.000000  0.000000  0.000000\n",
      "EART    250   39  336  3327   0.426621  0.865052  0.571429\n",
      "ECON     77   25  211  3639   0.267361  0.754902  0.394872\n",
      "ENER    227   41  432  3252   0.344461  0.847015  0.489752\n",
      "ENGI    500  121  660  2671   0.431034  0.805153  0.561482\n",
      "ENVI    501   98  529  2824   0.486408  0.836394  0.615101\n",
      "HEAL     43   31  303  3575   0.124277  0.581081  0.204762\n",
      "IMMU    249   66  351  3286   0.415000  0.790476  0.544262\n",
      "MATE    382   53  384  3133   0.498695  0.878161  0.636137\n",
      "MATH    120   48  422  3362   0.221402  0.714286  0.338028\n",
      "MEDI    781  126  401  2644   0.660745  0.861080  0.747726\n",
      "MULT     12   12   42  3886   0.222222  0.500000  0.307692\n",
      "NEUR    279   43  287  3343   0.492933  0.866460  0.628378\n",
      "NURS      7   15   55  3875   0.112903  0.318182  0.166667\n",
      "PHAR    205   47  433  3267   0.321317  0.813492  0.460674\n",
      "PHYS    371   66  503  3012   0.424485  0.848970  0.565980\n",
      "PSYC    127   33  304  3488   0.294664  0.793750  0.429780\n",
      "SOCI    280   63  439  3170   0.389430  0.816327  0.527307\n",
      "VETE     91   19   86  3756   0.514124  0.827273  0.634146\n",
      "{'f1': {'micro': 0.5275337475894579, 'macro': 0.43976673147796164, 'weighted': 0.5536780601440746}}\n",
      "finish fold-2\n",
      "x train: (31620, 1784)\n",
      "x val  : (3953, 1784)\n",
      "x test : (3952, 1784)\n",
      "y train: (31620, 27)\n",
      "y val  : (3953, 27)\n",
      "y test : (3952, 27)\n",
      "unique train: 332\n",
      "unique val  : 261\n",
      "unique test : 254\n",
      "prepare training fold-3 .... took a while\n",
      "elapsed time for training : 7640.801694 s\n",
      "fold-3 ...done training\n",
      "         TP   FN   FP    TN  Precision    Recall  f1-score\n",
      "labels                                                    \n",
      "AGRI    355   73  513  3011   0.408986  0.829439  0.547840\n",
      "ARTS     72   33  269  3578   0.211144  0.685714  0.322870\n",
      "BIOC    681  162  496  2613   0.578590  0.807829  0.674257\n",
      "BUSI     75   30  322  3525   0.188917  0.714286  0.298805\n",
      "CENG    175   45  573  3159   0.233957  0.795455  0.361570\n",
      "CHEM    198   60  562  3132   0.260526  0.767442  0.388998\n",
      "COMP    220   76  554  3102   0.284238  0.743243  0.411215\n",
      "DECI     42   18  200  3692   0.173554  0.700000  0.278146\n",
      "DENT      0    5    1  3946   0.000000  0.000000  0.000000\n",
      "EART    224   37  310  3381   0.419476  0.858238  0.563522\n",
      "ECON     85   29  256  3582   0.249267  0.745614  0.373626\n",
      "ENER    218   47  412  3275   0.346032  0.822642  0.487151\n",
      "ENGI    456  113  631  2752   0.419503  0.801406  0.550725\n",
      "ENVI    544  121  515  2772   0.513692  0.818045  0.631090\n",
      "HEAL     33   40  298  3581   0.099698  0.452055  0.163366\n",
      "IMMU    259   61  361  3271   0.417742  0.809375  0.551064\n",
      "MATE    341   54  381  3176   0.472299  0.863291  0.610564\n",
      "MATH    107   44  420  3381   0.203036  0.708609  0.315634\n",
      "MEDI    748  123  420  2661   0.640411  0.858783  0.733693\n",
      "MULT     16    9   33  3894   0.326531  0.640000  0.432432\n",
      "NEUR    287   44  298  3323   0.490598  0.867069  0.626638\n",
      "NURS     13   18   48  3873   0.213115  0.419355  0.282609\n",
      "PHAR    167   45  509  3231   0.247041  0.787736  0.376126\n",
      "PHYS    316   48  507  3081   0.383961  0.868132  0.532435\n",
      "PSYC    160   36  319  3437   0.334029  0.816327  0.474074\n",
      "SOCI    327   51  475  3099   0.407731  0.865079  0.554237\n",
      "VETE     73   19   86  3774   0.459119  0.793478  0.581673\n",
      "{'f1': {'micro': 0.5248792065779435, 'macro': 0.4490503895680166, 'weighted': 0.5516683807814351}}\n",
      "finish fold-3\n",
      "x train: (31620, 1784)\n",
      "x val  : (3953, 1784)\n",
      "x test : (3952, 1784)\n",
      "y train: (31620, 27)\n",
      "y val  : (3953, 27)\n",
      "y test : (3952, 27)\n",
      "unique train: 334\n",
      "unique val  : 255\n",
      "unique test : 266\n",
      "prepare training fold-4 .... took a while\n",
      "elapsed time for training : 7734.677297 s\n",
      "fold-4 ...done training\n",
      "         TP   FN   FP    TN  Precision    Recall  f1-score\n",
      "labels                                                    \n",
      "AGRI    291  100  495  3066   0.370229  0.744246  0.494477\n",
      "ARTS     65   37  269  3581   0.194611  0.637255  0.298165\n",
      "BIOC    690  175  496  2591   0.581788  0.797688  0.672843\n",
      "BUSI     72   31  288  3561   0.200000  0.699029  0.311015\n",
      "CENG    153   42  551  3206   0.217330  0.784615  0.340378\n",
      "CHEM    213   55  548  3136   0.279895  0.794776  0.413994\n",
      "COMP    231   73  533  3115   0.302356  0.759868  0.432584\n",
      "DECI     30   28  160  3734   0.157895  0.517241  0.241935\n",
      "DENT      0    2    0  3950   0.000000  0.000000  0.000000\n",
      "EART    237   40  323  3352   0.423214  0.855596  0.566308\n",
      "ECON     89   29  236  3598   0.273846  0.754237  0.401806\n",
      "ENER    245   42  411  3254   0.373476  0.853659  0.519618\n",
      "ENGI    510   90  598  2754   0.460289  0.850000  0.597190\n",
      "ENVI    490  109  533  2820   0.478983  0.818030  0.604192\n",
      "HEAL     44   39  290  3579   0.131737  0.530120  0.211031\n",
      "IMMU    264   61  395  3232   0.400607  0.812308  0.536585\n",
      "MATE    338   43  373  3198   0.475387  0.887139  0.619048\n",
      "MATH    117   39  433  3363   0.212727  0.750000  0.331445\n",
      "MEDI    729  141  443  2639   0.622014  0.837931  0.714006\n",
      "MULT     18   14   27  3893   0.400000  0.562500  0.467532\n",
      "NEUR    279   51  261  3361   0.516667  0.845455  0.641379\n",
      "NURS     14   11   55  3872   0.202899  0.560000  0.297872\n",
      "PHAR    165   47  450  3290   0.268293  0.778302  0.399033\n",
      "PHYS    378   46  511  3017   0.425197  0.891509  0.575781\n",
      "PSYC    127   29  348  3448   0.267368  0.814103  0.402536\n",
      "SOCI    315   42  495  3100   0.388889  0.882353  0.539846\n",
      "VETE     81   28   77  3766   0.512658  0.743119  0.606742\n",
      "{'f1': {'micro': 0.5283389569897066, 'macro': 0.4532348884219388, 'weighted': 0.5530728304079436}}\n",
      "finish fold-4\n",
      "x train: (31620, 1784)\n",
      "x val  : (3953, 1784)\n",
      "x test : (3952, 1784)\n",
      "y train: (31620, 27)\n",
      "y val  : (3953, 27)\n",
      "y test : (3952, 27)\n",
      "unique train: 331\n",
      "unique val  : 261\n",
      "unique test : 259\n",
      "prepare training fold-5 .... took a while\n",
      "elapsed time for training : 7662.379414 s\n",
      "fold-5 ...done training\n",
      "         TP   FN   FP    TN  Precision    Recall  f1-score\n",
      "labels                                                    \n",
      "AGRI    321   82  501  3048   0.390511  0.796526  0.524082\n",
      "ARTS     70   36  275  3571   0.202899  0.660377  0.310421\n",
      "BIOC    636  153  537  2626   0.542199  0.806084  0.648318\n",
      "BUSI     57   35  293  3567   0.162857  0.619565  0.257919\n",
      "CENG    151   57  580  3164   0.206566  0.725962  0.321619\n",
      "CHEM    203   45  537  3167   0.274324  0.818548  0.410931\n",
      "COMP    237   58  551  3106   0.300761  0.803390  0.437673\n",
      "DECI     26   32  201  3693   0.114537  0.448276  0.182456\n",
      "DENT      0    6    0  3946   0.000000  0.000000  0.000000\n",
      "EART    232   48  311  3361   0.427256  0.828571  0.563791\n",
      "ECON     77   30  227  3618   0.253289  0.719626  0.374696\n",
      "ENER    261   49  424  3218   0.381022  0.841935  0.524623\n",
      "ENGI    474  105  648  2725   0.422460  0.818653  0.557319\n",
      "ENVI    544  100  523  2785   0.509841  0.844720  0.635885\n",
      "HEAL     38   48  290  3576   0.115854  0.441860  0.183575\n",
      "IMMU    262   57  363  3270   0.419200  0.821317  0.555085\n",
      "MATE    346   59  339  3208   0.505109  0.854321  0.634862\n",
      "MATH    115   37  413  3387   0.217803  0.756579  0.338235\n",
      "MEDI    786  139  405  2622   0.659950  0.849730  0.742911\n",
      "MULT     15   15   25  3897   0.375000  0.500000  0.428571\n",
      "NEUR    292   32  318  3310   0.478689  0.901235  0.625268\n",
      "NURS     14   14   43  3881   0.245614  0.500000  0.329412\n",
      "PHAR    167   46  469  3270   0.262579  0.784038  0.393404\n",
      "PHYS    309   48  540  3055   0.363958  0.865546  0.512438\n",
      "PSYC    133   30  342  3447   0.280000  0.815951  0.416928\n",
      "SOCI    287   56  523  3086   0.354321  0.836735  0.497832\n",
      "VETE     77   10   99  3766   0.437500  0.885057  0.585551\n",
      "{'f1': {'micro': 0.5225025571087624, 'macro': 0.44421501728269125, 'weighted': 0.5491986754358273}}\n",
      "finish fold-5\n",
      "func:evaluate_svm \n",
      "Execution Time : 8293.041485 s\n",
      "{'result':          TP   FN   FP    TN  Precision    Recall  f1-score\n",
      "labels                                                    \n",
      "AGRI    319   87  503  3042   0.387706  0.784459  0.518632\n",
      "ARTS     63   36  269  3582   0.190652  0.634795  0.293133\n",
      "BIOC    661  166  505  2618   0.566854  0.799491  0.663223\n",
      "BUSI     64   30  288  3569   0.182914  0.678546  0.288097\n",
      "CENG    163   44  577  3167   0.220083  0.785052  0.343738\n",
      "CHEM    214   53  560  3124   0.276211  0.799400  0.410401\n",
      "COMP    225   67  539  3119   0.293900  0.767681  0.425024\n",
      "DECI     28   26  174  3722   0.137650  0.510465  0.216620\n",
      "DENT      0    4    0  3946   0.000000  0.000000  0.000000\n",
      "EART    236   41  323  3350   0.422849  0.851809  0.565103\n",
      "ECON     83   27  233  3608   0.263074  0.753434  0.389920\n",
      "ENER    235   47  411  3257   0.364370  0.832491  0.506586\n",
      "ENGI    485  104  628  2733   0.436035  0.822740  0.569950\n",
      "ENVI    520  105  529  2796   0.495820  0.831118  0.620985\n",
      "HEAL     39   39  292  3580   0.119158  0.499789  0.192249\n",
      "IMMU    259   57  373  3261   0.410510  0.818682  0.546603\n",
      "MATE    348   54  372  3177   0.483315  0.864341  0.619799\n",
      "MATH    115   40  421  3373   0.215356  0.740733  0.333634\n",
      "MEDI    763  136  410  2642   0.650130  0.848750  0.736151\n",
      "MULT     14   12   32  3893   0.313639  0.540500  0.392917\n",
      "NEUR    285   41  294  3330   0.492444  0.873958  0.629603\n",
      "NURS     13   14   51  3872   0.203555  0.465390  0.281979\n",
      "PHAR    173   46  461  3271   0.273616  0.789651  0.405945\n",
      "PHYS    340   53  514  3043   0.397232  0.863307  0.543689\n",
      "PSYC    136   31  328  3456   0.293654  0.813670  0.431064\n",
      "SOCI    302   53  487  3109   0.383322  0.850407  0.528222\n",
      "VETE     82   20   84  3764   0.493114  0.807680  0.610194, 'f1': {'f1': {'micro': 0.5264506212373511, 'macro': 0.44679479959555457, 'weighted': 0.5521947722279626}}}\n",
      "AVERAGE PRECISION :  0.33211710477286605\n",
      "AVERAGE RECALL :  0.7269755722495908\n"
     ]
    }
   ],
   "source": [
    "# x = _eval_svm()\n",
    "svm_result = _eval_svm(save_file=False)\n",
    "print(svm_result)\n",
    "\n",
    "print(\"AVERAGE PRECISION : \", svm_result['result']['Precision'].mean())\n",
    "print(\"AVERAGE RECALL : \", svm_result['result']['Recall'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0e91a40b-6c0f-4ea5-964d-f69cbd8e92f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes\n",
    "\n",
    "def _eval_nb(save_file=False) :\n",
    "    f_nbstart = datetime.now()\n",
    "    i = 1\n",
    "    t_res = []\n",
    "    list_dataframe = []\n",
    "    for train_index, val_index, test_index in fold :\n",
    "        f_fold_nb = datetime.now()\n",
    "        \n",
    "        \n",
    "        X_train = vectorizer.transform(shuff['clean_text'][train_index]).toarray()\n",
    "        X_val = vectorizer.transform(shuff['clean_text'][val_index]).toarray()\n",
    "        X_test = vectorizer.transform(shuff['clean_text'][test_index]).toarray()\n",
    "\n",
    "\n",
    "        y_train = np.array(shuff['label_cat'][train_index].tolist()).ravel()\n",
    "        y_train = np.reshape(y_train, (int(y_train.shape[0]/class_num), class_num))\n",
    "        \n",
    "        y_val = np.array(shuff['label_cat'][val_index].tolist()).ravel()\n",
    "        y_val = np.reshape(y_val, (int(y_val.shape[0]/class_num), class_num))\n",
    "\n",
    "        y_test = np.array(shuff['label_cat'][test_index].tolist()).ravel()\n",
    "        y_test = np.reshape(y_test, (int(y_test.shape[0]/class_num), class_num))\n",
    "        \n",
    "        t_time = datetime.now()\n",
    "        print(\"prepare training fold-{} .... took a while\".format(i))\n",
    "        mb_nb = MultinomialNB(alpha = 0.6)\n",
    "        clf = MultiOutputClassifier(mb_nb).fit(X_train, y_train)\n",
    "        t_dur = datetime.now() - t_time  \n",
    "        print('elapsed time for training : {:2f} s'.format(t_dur.total_seconds()))\n",
    "    \n",
    "        pred = clf.predict(X_test)\n",
    "        \n",
    "        result_df = _report_classification(multilabel_confusion_matrix(y_test, pred))\n",
    "        \n",
    "        print(result_df)\n",
    "        print(get_f1(result_df))\n",
    "        list_dataframe.append(result_df)\n",
    "        # delete\n",
    "        X_train, X_val,X_test = None, None, None\n",
    "        y_train, y_val,y_test = None, None, None\n",
    "        \n",
    "        print('finish fold-{}'.format(i))\n",
    "        i += 1\n",
    "    # average all fold\n",
    "    result_avg = _average_dataframe(list_dataframe)\n",
    "    result_f1 = get_f1(result_avg)\n",
    "\n",
    "    f_nbduration = datetime.now() - f_nbstart\n",
    "    print(\"func:evaluate_naivebayes \\nExecution Time : {:2f} s\".format(f_nbduration.total_seconds()))\n",
    "    return {'result' : result_avg,\n",
    "            'f1'     : result_f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ea0fddba-9103-429d-b919-3b45d5478c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare training fold-1 .... took a while\n",
      "elapsed time for training : 3.328988 s\n",
      "         TP   FN   FP    TN  Precision    Recall  f1-score\n",
      "labels                                                    \n",
      "AGRI     94  343   48  3467   0.661972  0.215103  0.324698\n",
      "ARTS      2   96    0  3854   1.000000  0.020408  0.040000\n",
      "BIOC    461  376  228  2887   0.669086  0.550777  0.604194\n",
      "BUSI      5   83   14  3850   0.263158  0.056818  0.093458\n",
      "CENG     36  163   21  3732   0.631579  0.180905  0.281250\n",
      "CHEM     61  240   71  3580   0.462121  0.202658  0.281755\n",
      "COMP     88  216   35  3613   0.715447  0.289474  0.412178\n",
      "DECI      0   47    0  3905   0.000000  0.000000  0.000000\n",
      "DENT      0    8    0  3944   0.000000  0.000000  0.000000\n",
      "EART    137  146   41  3628   0.769663  0.484099  0.594360\n",
      "ECON     12   99   14  3827   0.461538  0.108108  0.175182\n",
      "ENER     94  192   37  3629   0.717557  0.328671  0.450839\n",
      "ENGI    283  299  189  3181   0.599576  0.486254  0.537002\n",
      "ENVI    315  310  176  3151   0.641548  0.504000  0.564516\n",
      "HEAL      0   81    0  3871   0.000000  0.000000  0.000000\n",
      "IMMU    140  167   58  3587   0.707071  0.456026  0.554455\n",
      "MATE    234  163  118  3437   0.664773  0.589421  0.624833\n",
      "MATH     28  127   15  3782   0.651163  0.180645  0.282828\n",
      "MEDI    622  300  205  2825   0.752116  0.674620  0.711264\n",
      "MULT      0   22    0  3930   0.000000  0.000000  0.000000\n",
      "NEUR    156  170   33  3593   0.825397  0.478528  0.605825\n",
      "NURS      1   33    1  3917   0.500000  0.029412  0.055556\n",
      "PHAR     53  156   47  3696   0.530000  0.253589  0.343042\n",
      "PHYS    206  181  195  3370   0.513716  0.532300  0.522843\n",
      "PSYC     52  111   48  3741   0.520000  0.319018  0.395437\n",
      "SOCI    181  176  123  3472   0.595395  0.507003  0.547655\n",
      "VETE     49   65   28  3810   0.636364  0.429825  0.513089\n",
      "{'f1': {'micro': 0.519827247742442, 'macro': 0.3524540755121965, 'weighted': 0.4970779556358085}}\n",
      "finish fold-1\n",
      "prepare training fold-2 .... took a while\n",
      "elapsed time for training : 3.112208 s\n",
      "         TP   FN   FP    TN  Precision    Recall  f1-score\n",
      "labels                                                    \n",
      "AGRI    101  274   43  3534   0.701389  0.269333  0.389210\n",
      "ARTS      0   88    0  3864   0.000000  0.000000  0.000000\n",
      "BIOC    420  385  256  2891   0.621302  0.521739  0.567184\n",
      "BUSI      8   79   20  3845   0.285714  0.091954  0.139130\n",
      "CENG     35  181   34  3702   0.507246  0.162037  0.245614\n",
      "CHEM     68  195   79  3610   0.462585  0.258555  0.331707\n",
      "COMP     69  196   28  3659   0.711340  0.260377  0.381215\n",
      "DECI      0   50    0  3902   0.000000  0.000000  0.000000\n",
      "DENT      0    3    0  3949   0.000000  0.000000  0.000000\n",
      "EART    129  160   21  3642   0.860000  0.446367  0.587699\n",
      "ECON      8   94   19  3831   0.296296  0.078431  0.124031\n",
      "ENER    104  164   38  3646   0.732394  0.388060  0.507317\n",
      "ENGI    282  339  214  3117   0.568548  0.454106  0.504924\n",
      "ENVI    295  304  173  3180   0.630342  0.492487  0.552952\n",
      "HEAL      0   74    0  3878   0.000000  0.000000  0.000000\n",
      "IMMU    137  178   54  3583   0.717277  0.434921  0.541502\n",
      "MATE    286  149  111  3406   0.720403  0.657471  0.687500\n",
      "MATH     29  139   14  3770   0.674419  0.172619  0.274882\n",
      "MEDI    629  278  210  2835   0.749702  0.693495  0.720504\n",
      "MULT      0   24    0  3928   0.000000  0.000000  0.000000\n",
      "NEUR    133  189   36  3594   0.786982  0.413043  0.541752\n",
      "NURS      0   22    0  3930   0.000000  0.000000  0.000000\n",
      "PHAR     59  193   39  3661   0.602041  0.234127  0.337143\n",
      "PHYS    222  215  174  3341   0.560606  0.508009  0.533013\n",
      "PSYC     48  112   37  3755   0.564706  0.300000  0.391837\n",
      "SOCI    168  175  122  3487   0.579310  0.489796  0.530806\n",
      "VETE     62   48   23  3819   0.729412  0.563636  0.635897\n",
      "{'f1': {'micro': 0.5210097333227823, 'macro': 0.3528081505105566, 'weighted': 0.49941331139379663}}\n",
      "finish fold-2\n",
      "prepare training fold-3 .... took a while\n",
      "elapsed time for training : 3.099974 s\n",
      "         TP   FN   FP    TN  Precision    Recall  f1-score\n",
      "labels                                                    \n",
      "AGRI     99  329   52  3472   0.655629  0.231308  0.341969\n",
      "ARTS      2  103    0  3847   1.000000  0.019048  0.037383\n",
      "BIOC    468  375  260  2849   0.642857  0.555160  0.595799\n",
      "BUSI      4  101   27  3820   0.129032  0.038095  0.058824\n",
      "CENG     31  189   26  3706   0.543860  0.140909  0.223827\n",
      "CHEM     62  196   91  3603   0.405229  0.240310  0.301703\n",
      "COMP     78  218   39  3617   0.666667  0.263514  0.377724\n",
      "DECI      0   60    0  3892   0.000000  0.000000  0.000000\n",
      "DENT      0    5    0  3947   0.000000  0.000000  0.000000\n",
      "EART    115  146   29  3662   0.798611  0.440613  0.567901\n",
      "ECON     11  103   21  3817   0.343750  0.096491  0.150685\n",
      "ENER    112  153   39  3648   0.741722  0.422642  0.538462\n",
      "ENGI    255  314  184  3199   0.580866  0.448155  0.505952\n",
      "ENVI    323  342  161  3126   0.667355  0.485714  0.562228\n",
      "HEAL      0   73    0  3879   0.000000  0.000000  0.000000\n",
      "IMMU    136  184   53  3579   0.719577  0.425000  0.534381\n",
      "MATE    238  157  135  3422   0.638070  0.602532  0.619792\n",
      "MATH     21  130   21  3780   0.500000  0.139073  0.217617\n",
      "MEDI    560  311  213  2868   0.724450  0.642939  0.681265\n",
      "MULT      0   25    0  3927   0.000000  0.000000  0.000000\n",
      "NEUR    158  173   44  3577   0.782178  0.477341  0.592871\n",
      "NURS      0   31    0  3921   0.000000  0.000000  0.000000\n",
      "PHAR     55  157   62  3678   0.470085  0.259434  0.334347\n",
      "PHYS    200  164  168  3420   0.543478  0.549451  0.546448\n",
      "PSYC     58  138   40  3716   0.591837  0.295918  0.394558\n",
      "SOCI    198  180  126  3448   0.611111  0.523810  0.564103\n",
      "VETE     45   47   13  3847   0.775862  0.489130  0.600000\n",
      "{'f1': {'micro': 0.5098689404705511, 'macro': 0.34621616845720854, 'weighted': 0.4875312670797983}}\n",
      "finish fold-3\n",
      "prepare training fold-4 .... took a while\n",
      "elapsed time for training : 3.134849 s\n",
      "         TP   FN   FP    TN  Precision    Recall  f1-score\n",
      "labels                                                    \n",
      "AGRI     83  308   52  3509   0.614815  0.212276  0.315589\n",
      "ARTS      0  102    1  3849   0.000000  0.000000  0.000000\n",
      "BIOC    458  407  252  2835   0.645070  0.529480  0.581587\n",
      "BUSI      8   95   20  3829   0.285714  0.077670  0.122137\n",
      "CENG     25  170   19  3738   0.568182  0.128205  0.209205\n",
      "CHEM     51  217   79  3605   0.392308  0.190299  0.256281\n",
      "COMP     86  218   40  3608   0.682540  0.282895  0.400000\n",
      "DECI      0   58    0  3894   0.000000  0.000000  0.000000\n",
      "DENT      0    2    0  3950   0.000000  0.000000  0.000000\n",
      "EART    136  141   26  3649   0.839506  0.490975  0.619590\n",
      "ECON     10  108   10  3824   0.500000  0.084746  0.144928\n",
      "ENER    100  187   41  3624   0.709220  0.348432  0.467290\n",
      "ENGI    285  315  207  3145   0.579268  0.475000  0.521978\n",
      "ENVI    314  285  148  3205   0.679654  0.524207  0.591894\n",
      "HEAL      0   83    0  3869   0.000000  0.000000  0.000000\n",
      "IMMU    139  186   57  3570   0.709184  0.427692  0.533589\n",
      "MATE    239  142  115  3456   0.675141  0.627297  0.650340\n",
      "MATH     27  129   23  3773   0.540000  0.173077  0.262136\n",
      "MEDI    599  271  218  2864   0.733170  0.688506  0.710136\n",
      "MULT      0   32    0  3920   0.000000  0.000000  0.000000\n",
      "NEUR    157  173   34  3588   0.821990  0.475758  0.602687\n",
      "NURS      0   25    3  3924   0.000000  0.000000  0.000000\n",
      "PHAR     49  163   49  3691   0.500000  0.231132  0.316129\n",
      "PHYS    215  209  162  3366   0.570292  0.507075  0.536829\n",
      "PSYC     41  115   41  3755   0.500000  0.262821  0.344538\n",
      "SOCI    169  188  135  3460   0.555921  0.473389  0.511346\n",
      "VETE     51   58   26  3817   0.662338  0.467890  0.548387\n",
      "{'f1': {'micro': 0.5134214902209201, 'macro': 0.3424666044950519, 'weighted': 0.48895017529479357}}\n",
      "finish fold-4\n",
      "prepare training fold-5 .... took a while\n",
      "elapsed time for training : 3.261896 s\n",
      "         TP   FN   FP    TN  Precision    Recall  f1-score\n",
      "labels                                                    \n",
      "AGRI    115  288   45  3504   0.718750  0.285360  0.408526\n",
      "ARTS      1  105    3  3843   0.250000  0.009434  0.018182\n",
      "BIOC    426  363  266  2897   0.615607  0.539924  0.575287\n",
      "BUSI      8   84   22  3838   0.266667  0.086957  0.131148\n",
      "CENG     22  186   33  3711   0.400000  0.105769  0.167300\n",
      "CHEM     62  186   68  3636   0.476923  0.250000  0.328042\n",
      "COMP     94  201   38  3619   0.712121  0.318644  0.440281\n",
      "DECI      0   58    0  3894   0.000000  0.000000  0.000000\n",
      "DENT      0    6    0  3946   0.000000  0.000000  0.000000\n",
      "EART    114  166   37  3635   0.754967  0.407143  0.529002\n",
      "ECON      6  101   11  3834   0.352941  0.056075  0.096774\n",
      "ENER     95  215   43  3599   0.688406  0.306452  0.424107\n",
      "ENGI    255  324  227  3146   0.529046  0.440415  0.480679\n",
      "ENVI    342  302  136  3172   0.715481  0.531056  0.609626\n",
      "HEAL      0   86    0  3866   0.000000  0.000000  0.000000\n",
      "IMMU    130  189   50  3583   0.722222  0.407524  0.521042\n",
      "MATE    240  165  112  3435   0.681818  0.592593  0.634082\n",
      "MATH     30  122   15  3785   0.666667  0.197368  0.304569\n",
      "MEDI    625  300  210  2817   0.748503  0.675676  0.710227\n",
      "MULT      0   30    0  3922   0.000000  0.000000  0.000000\n",
      "NEUR    170  154   50  3578   0.772727  0.524691  0.625000\n",
      "NURS      1   27    1  3923   0.500000  0.035714  0.066667\n",
      "PHAR     54  159   59  3680   0.477876  0.253521  0.331288\n",
      "PHYS    188  169  203  3392   0.480818  0.526611  0.502674\n",
      "PSYC     62  101   44  3745   0.584906  0.380368  0.460967\n",
      "SOCI    149  194  144  3465   0.508532  0.434402  0.468553\n",
      "VETE     48   39   20  3845   0.705882  0.551724  0.619355\n",
      "{'f1': {'micro': 0.5125484918058745, 'macro': 0.3501250809561689, 'weighted': 0.4909114319921435}}\n",
      "finish fold-5\n",
      "func:evaluate_naivebayes \n",
      "Execution Time : 71.520230 s\n"
     ]
    }
   ],
   "source": [
    "nb_result = _eval_nb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3391ae9b-9693-4998-ae39-dfc1a12662d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'result':          TP   FN   FP    TN  Precision    Recall  f1-score\n",
      "labels                                                    \n",
      "AGRI     98  308   48  3497   0.670511  0.242676  0.355998\n",
      "ARTS      1   98    0  3851   0.450000  0.009778  0.019113\n",
      "BIOC    446  381  252  2871   0.638784  0.539416  0.584810\n",
      "BUSI      6   88   20  3836   0.246057  0.070299  0.108939\n",
      "CENG     29  177   26  3717   0.530173  0.143565  0.225439\n",
      "CHEM     60  206   77  3606   0.439833  0.228364  0.299898\n",
      "COMP     83  209   36  3623   0.697623  0.282981  0.402280\n",
      "DECI      0   54    0  3897   0.000000  0.000000  0.000000\n",
      "DENT      0    4    0  3947   0.000000  0.000000  0.000000\n",
      "EART    126  151   30  3643   0.804549  0.453839  0.579711\n",
      "ECON      9  101   15  3826   0.390905  0.084770  0.138320\n",
      "ENER    101  182   39  3629   0.717860  0.358851  0.477603\n",
      "ENGI    272  318  204  3157   0.571461  0.460786  0.510107\n",
      "ENVI    317  308  158  3166   0.666876  0.507493  0.576243\n",
      "HEAL      0   79    0  3872   0.000000  0.000000  0.000000\n",
      "IMMU    136  180   54  3580   0.715066  0.430233  0.536994\n",
      "MATE    247  155  118  3431   0.676041  0.613863  0.643309\n",
      "MATH     27  129   17  3778   0.606450  0.172556  0.268406\n",
      "MEDI    607  292  211  2841   0.741588  0.675047  0.706679\n",
      "MULT      0   26    0  3925   0.000000  0.000000  0.000000\n",
      "NEUR    154  171   39  3586   0.797855  0.473872  0.593627\n",
      "NURS      0   27    1  3923   0.200000  0.013025  0.024444\n",
      "PHAR     54  165   51  3681   0.516000  0.246361  0.332390\n",
      "PHYS    206  187  180  3377   0.533782  0.524689  0.528361\n",
      "PSYC     52  115   42  3742   0.552290  0.311625  0.397467\n",
      "SOCI    173  182  130  3466   0.570054  0.485680  0.524493\n",
      "VETE     51   51   22  3827   0.701971  0.500441  0.583346, 'f1': {'f1': {'micro': 0.5156844106463878, 'macro': 0.34881401598623657, 'weighted': 0.49317469819561977}}}\n"
     ]
    }
   ],
   "source": [
    "print(nb_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a894654c-97ce-41e9-99a3-5f7bdc333f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE PRECISION :  0.4976196525964238\n",
      "AVERAGE RECALL :  0.2900077932455214\n"
     ]
    }
   ],
   "source": [
    "print(\"AVERAGE PRECISION : \", nb_result['result']['Precision'].mean())\n",
    "print(\"AVERAGE RECALL : \", nb_result['result']['Recall'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03647102-fde2-4e60-93d9-283f40d40173",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Performance Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867b5b10-d2de-46a0-9399-d0926ad164f2",
   "metadata": {},
   "source": [
    "### Training History"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e00fc4b-2a9e-4e63-b2fd-c19dc0f91f13",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Vanilla Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6646d58-13fd-4633-ac6f-5b0e8e017e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print dict keys, what does history contain\n",
    "# print(vanilla_measurement['history'][0].history.keys())\n",
    "# dont print figure automaticly\n",
    "_i = plt.ioff()\n",
    "for i in range(len(vanilla_measurement['history'])) :\n",
    "    # print(vanilla_measurement['history'][i].history.keys())\n",
    "    fig, ((ax1, ax2)) = plt.subplots(1, 2, figsize=(10,3))\n",
    "    ax1.plot(vanilla_measurement['history'][i].history['loss'], label=\"loss\")\n",
    "    ax1.plot(vanilla_measurement['history'][i].history['val_loss'], label=\"val_loss\")\n",
    "    ax1.legend(fontsize='x-small')\n",
    "    # Add a footnote below and to the right side of the chart\n",
    "    ax1.annotate('*lower is better',\n",
    "                xy = (1.0, -0.15),\n",
    "                xycoords='axes fraction',\n",
    "                ha='right',\n",
    "                va=\"center\",\n",
    "                fontsize=7)\n",
    "\n",
    "    ax2.plot(vanilla_measurement['history'][i].history['binary_accuracy'], label=\"binary_accuracy\")\n",
    "    ax2.plot(vanilla_measurement['history'][i].history['val_binary_accuracy'], label=\"val_binary_accuracy\")\n",
    "    # ax2.plot(vanilla_measurement['history'][i].history['val_binary_accuracy'], label=\"val_binary_accuracy\")\n",
    "    ax2.legend(fontsize='x-small')\n",
    "    \n",
    "    ax2.set_yticks(np.arange(0,1.2,0.1))\n",
    "    ax2.set_xlim(left=-5, right=104)\n",
    "    \n",
    "    # max accuracy\n",
    "    ax2.plot(np.arange(-10,110,1), np.ones(120), color='red')\n",
    "    # Add a footnote below and to the right side of the chart\n",
    "    ax2.annotate('*higher is better',\n",
    "                xy = (1.0, -0.15),\n",
    "                xycoords='axes fraction',\n",
    "                ha='right',\n",
    "                va=\"center\",\n",
    "                fontsize=7)\n",
    "\n",
    "    # fig.suptitle(\"Fold {} : Vanilla Performance\".format(i+1))\n",
    "    fig.suptitle(\"Fold {} : Baseline Performance\".format(i+1))\n",
    "    f_name = \"{}img{}multi_base_perf_f{}.png\".format(RESULTS, sep, i+1)\n",
    "    fig.savefig(f_name, bbox_inches =\"tight\",\n",
    "                        pad_inches = 0.15)\n",
    "    display(fig)\n",
    "\n",
    "# close all fig after printed\n",
    "plt.close('all')\n",
    "_i = plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871d814d-c38e-496b-8def-712912a99861",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Encoded Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdac885c-14a3-4a14-bbd6-855cb47bd55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dont print figure automaticly\n",
    "_i = plt.ioff()\n",
    "for i in range(len(encoded_measurement['history'])) :\n",
    "    # print(vanilla_measurement['history'][i].history.keys())\n",
    "    fig, ((ax1, ax2)) = plt.subplots(1, 2, figsize=(10,3))\n",
    "    ax1.plot(encoded_measurement['history'][i].history['loss'], label=\"loss\")\n",
    "    ax1.plot(encoded_measurement['history'][i].history['val_loss'], label=\"val_loss\")\n",
    "    ax1.legend(fontsize='x-small')\n",
    "    # Add a footnote below and to the right side of the chart\n",
    "    ax1.annotate('*lower is better',\n",
    "                xy = (1.0, -0.15),\n",
    "                xycoords='axes fraction',\n",
    "                ha='right',\n",
    "                va=\"center\",\n",
    "                fontsize=7)\n",
    "\n",
    "    ax2.plot(encoded_measurement['history'][i].history['binary_accuracy'], label=\"binary_accuracy\")\n",
    "    ax2.plot(encoded_measurement['history'][i].history['val_binary_accuracy'], label=\"val_binary_accuracy\")\n",
    "    ax2.legend(fontsize='x-small')\n",
    "    \n",
    "    ax2.set_yticks(np.arange(0,1.2,0.1))\n",
    "    ax2.set_xlim(left=-5, right=104)\n",
    "    \n",
    "    # max accuracy\n",
    "    ax2.plot(np.arange(-10,110,1), np.ones(120), color='red')\n",
    "    # Add a footnote below and to the right side of the chart\n",
    "    ax2.annotate('*higher is better',\n",
    "                xy = (1.0, -0.15),\n",
    "                xycoords='axes fraction',\n",
    "                ha='right',\n",
    "                va=\"center\",\n",
    "                fontsize=7)\n",
    "\n",
    "    fig.suptitle(\"Fold {} : Encoded Performance\".format(i+1))\n",
    "    f_name = \"{}img{}multi_encoded_perf_f{}.png\".format(RESULTS, sep, i+1)\n",
    "    fig.savefig(f_name, bbox_inches =\"tight\",\n",
    "                        pad_inches = 0.15)\n",
    "    display(fig)\n",
    "\n",
    "# close all fig after printed\n",
    "plt.close('all')\n",
    "_i = plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c794c61-d955-498a-a435-e28e393dbd38",
   "metadata": {},
   "source": [
    "#### Training History Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d88a60-dcbe-4856-ba9c-1403202df1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dont print figure automaticly\n",
    "_i = plt.ioff()\n",
    "for i in range(len(encoded_measurement['history'])) :\n",
    "    fig, ((ax1, ax2)) = plt.subplots(1, 2, figsize=(10,3))\n",
    "    \n",
    "    ax1.plot(vanilla_measurement['history'][i].history['val_loss'], label=\"base loss\")\n",
    "    ax1.plot(encoded_measurement['history'][i].history['val_loss'], label=\"encoded loss\")\n",
    "\n",
    "    # ax1.plot(vanilla_measurement['history'][i].history['val_loss'], label=\"vanilla loss\")\n",
    "    ax1.legend(fontsize='x-small')\n",
    "    # Add a footnote below and to the right side of the chart\n",
    "    ax1.annotate('*lower is better',\n",
    "                xy = (1.0, -0.15),\n",
    "                xycoords='axes fraction',\n",
    "                ha='right',\n",
    "                va=\"center\",\n",
    "                fontsize=7)\n",
    "\n",
    "    ax2.plot(vanilla_measurement['history'][i].history['val_binary_accuracy'], label=\"base_binary accuracy\")\n",
    "    ax2.plot(encoded_measurement['history'][i].history['val_binary_accuracy'], label=\"encoded_binary accuracy\")\n",
    "\n",
    "    # ax2.plot(vanilla_measurement['history'][i].history['val_binary_accuracy'], label=\"vanila_binary accuracy\")\n",
    "    ax2.legend(fontsize='x-small')\n",
    "    \n",
    "    ax2.set_yticks(np.arange(0,1.2,0.1))\n",
    "    ax2.set_xlim(left=-5, right=104)\n",
    "    \n",
    "    # max accuracy\n",
    "    ax2.plot(np.arange(-10,110,1), np.ones(120), color='red')\n",
    "    # Add a footnote below and to the right side of the chart\n",
    "    ax2.annotate('*higher is better',\n",
    "                xy = (1.0, -0.15),\n",
    "                xycoords='axes fraction',\n",
    "                ha='right',\n",
    "                va=\"center\",\n",
    "                fontsize=7)\n",
    "    \n",
    "\n",
    "    fig.suptitle(\"Fold {} : Validation Performance Comparisson\".format(i+1))\n",
    "    f_name = \"{}img{}multi_val_comp_f{}.png\".format(RESULTS, sep, i+1)\n",
    "    fig.savefig(f_name, bbox_inches =\"tight\",\n",
    "                        pad_inches = 0.15)\n",
    "    # fig.tight_layout()\n",
    "    display(fig)\n",
    "\n",
    "# close all fig after printed\n",
    "plt.close('all')\n",
    "_i = plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9eb5a63-a062-4e8a-8b3f-5867aad31c07",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09160ec8-2647-426b-8cc1-7dc5a2e8c47b",
   "metadata": {},
   "source": [
    "### f1-micro comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8695206-5172-4de1-b2ad-e9336ffc4fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comaprison on f1-micro\n",
    "vanila_micro = []\n",
    "encoded_micro = []\n",
    "\n",
    "vanila_macro = []\n",
    "encoded_macro = []\n",
    "\n",
    "vanila_weight = []\n",
    "encoded_weight = []\n",
    "for i in range(5) :\n",
    "    vanila_micro.append(vanilla_measurement['metric'][i]['f1']['micro'])\n",
    "    encoded_micro.append(encoded_measurement['metric'][i]['f1']['micro'])\n",
    "\n",
    "\n",
    "_i = plt.ioff()\n",
    "\n",
    "# set width of bar\n",
    "barWidth = 0.25\n",
    "fig, ax1 = plt.subplots(1, 1, figsize=(10,3))\n",
    "\n",
    "# Set position of bar on X axis\n",
    "br1 = np.arange(len(vanila_micro))\n",
    "br2 = [x + barWidth for x in br1]\n",
    "\n",
    "# Make the plot\n",
    "# ax1.bar(br1, vanila_micro, width = barWidth,\n",
    "#         edgecolor ='grey', label ='vanila')\n",
    "ax1.bar(br1, vanila_micro, width = barWidth,\n",
    "        edgecolor ='grey', label ='base')\n",
    "ax1.bar(br2, encoded_micro, width = barWidth,\n",
    "        edgecolor ='grey', label ='encoded')\n",
    "\n",
    "\n",
    "ax1.legend(fontsize='x-small')\n",
    "\n",
    "plt.ylabel('f1-score', fontsize = 11)\n",
    "plt.xticks([r + barWidth for r in range(len(vanila_micro))],\n",
    "        ['fold 1', 'fold 2', 'fold 3', 'fold 4', 'fold 5'])\n",
    "fig.suptitle(\"f1-micro comparison\")\n",
    "# fig.tight_layout()\n",
    "display(fig)\n",
    "\n",
    "f_name = \"{}img{}multi_f1_micro_comp.png\".format(RESULTS, sep)\n",
    "fig.savefig(f_name, bbox_inches =\"tight\",\n",
    "                        pad_inches = 0.15)\n",
    "\n",
    "# close all fig after printed\n",
    "plt.close('all')\n",
    "_i = plt.ion()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c1e7d2-bd65-4c83-9f35-934c7510fe3e",
   "metadata": {},
   "source": [
    "### f1-macro comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244a4834-cbac-4df7-bf7a-0390f37cf310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comaprison on f1-macro\n",
    "\n",
    "\n",
    "vanila_macro = []\n",
    "encoded_macro = []\n",
    "\n",
    "for i in range(5) :\n",
    "    vanila_macro.append(vanilla_measurement['metric'][i]['f1']['macro'])\n",
    "    encoded_macro.append(encoded_measurement['metric'][i]['f1']['macro'])\n",
    "\n",
    "_i = plt.ioff()\n",
    "\n",
    "# set width of bar\n",
    "barWidth = 0.25\n",
    "fig, ax1 = plt.subplots(1, 1, figsize=(10,3))\n",
    "\n",
    "# Set position of bar on X axis\n",
    "br1 = np.arange(len(vanila_macro))\n",
    "br2 = [x + barWidth for x in br1]\n",
    "\n",
    "# Make the plot\n",
    "# ax1.bar(br1, vanila_macro, width = barWidth,\n",
    "#         edgecolor ='grey', label ='vanila')\n",
    "ax1.bar(br1, vanila_macro, width = barWidth,\n",
    "        edgecolor ='grey', label ='base')\n",
    "ax1.bar(br2, encoded_macro, width = barWidth,\n",
    "        edgecolor ='grey', label ='encoded')\n",
    "\n",
    "\n",
    "ax1.legend(fontsize='x-small')\n",
    "\n",
    "plt.ylabel('f1-score', fontsize = 11)\n",
    "plt.xticks([r + barWidth for r in range(len(vanila_micro))],\n",
    "        ['fold 1', 'fold 2', 'fold 3', 'fold 4', 'fold 5'])\n",
    "fig.suptitle(\"f1-macro comparison\")\n",
    "# fig.tight_layout()\n",
    "display(fig)\n",
    "\n",
    "f_name = \"{}img{}multi_f1_macro_comp.png\".format(RESULTS, sep)\n",
    "fig.savefig(f_name, bbox_inches =\"tight\",\n",
    "                        pad_inches = 0.15)\n",
    "\n",
    "# close all fig after printed\n",
    "plt.close('all')\n",
    "_i = plt.ion()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2ce079-bf3b-4264-9f1d-164521e3e200",
   "metadata": {},
   "source": [
    "### f1-weighted comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b3ef97-69cd-4883-bbb7-958e76252c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comaprison on f1-weighted\n",
    "\n",
    "vanila_weight = []\n",
    "encoded_weight = []\n",
    "for i in range(5) :\n",
    "    vanila_weight.append(vanilla_measurement['metric'][i]['f1']['weighted'])\n",
    "    encoded_weight.append(encoded_measurement['metric'][i]['f1']['weighted'])\n",
    "\n",
    "_i = plt.ioff()\n",
    "\n",
    "# set width of bar\n",
    "barWidth = 0.25\n",
    "fig, ax1 = plt.subplots(1, 1, figsize=(10,3))\n",
    "\n",
    "# Set position of bar on X axis\n",
    "br1 = np.arange(len(vanila_weight))\n",
    "br2 = [x + barWidth for x in br1]\n",
    "\n",
    "# Make the plot\n",
    "# ax1.bar(br1, vanila_weight, width = barWidth,\n",
    "#         edgecolor ='grey', label ='vanila')\n",
    "ax1.bar(br1, vanila_weight, width = barWidth,\n",
    "        edgecolor ='grey', label ='base')\n",
    "ax1.bar(br2, encoded_weight, width = barWidth,\n",
    "        edgecolor ='grey', label ='encoded')\n",
    "\n",
    "\n",
    "ax1.legend(fontsize='x-small')\n",
    "\n",
    "plt.ylabel('f1-score', fontsize = 11)\n",
    "plt.xticks([r + barWidth for r in range(len(vanila_weight))],\n",
    "        ['fold 1', 'fold 2', 'fold 3', 'fold 4', 'fold 5'])\n",
    "fig.suptitle(\"f1-weighted comparison\")\n",
    "# fig.tight_layout()\n",
    "display(fig)\n",
    "\n",
    "f_name = \"{}img{}multi_f1_weight_comp.png\".format(RESULTS, sep)\n",
    "fig.savefig(f_name, bbox_inches =\"tight\",\n",
    "                        pad_inches = 0.15)\n",
    "\n",
    "# close all fig after printed\n",
    "plt.close('all')\n",
    "_i = plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b074019d-51f1-462a-b994-591d65da221f",
   "metadata": {},
   "source": [
    "### Hamming Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fef9d2-5cd7-43d3-a472-0f68aed3af8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparison on hamming loss\n",
    "# add foot note lower is better\n",
    "\n",
    "vanila_hloss = []\n",
    "encoded_hloss= []\n",
    "for i in range(5) :\n",
    "    vanila_hloss.append(vanilla_measurement['metric'][i]['hamming_loss'])\n",
    "    encoded_hloss.append(encoded_measurement['metric'][i]['hamming_loss'])\n",
    "\n",
    "_i = plt.ioff()\n",
    "\n",
    "# set width of bar\n",
    "barWidth = 0.25\n",
    "fig, ax1 = plt.subplots(1, 1, figsize=(10,3))\n",
    "\n",
    "# Set position of bar on X axis\n",
    "br1 = np.arange(len(vanila_hloss))\n",
    "br2 = [x + barWidth for x in br1]\n",
    "\n",
    "# Make the plot\n",
    "# ax1.bar(br1, vanila_hloss, width = barWidth,\n",
    "#         edgecolor ='grey', label ='vanila')\n",
    "ax1.bar(br1, vanila_hloss, width = barWidth,\n",
    "        edgecolor ='grey', label ='base')\n",
    "ax1.bar(br2, encoded_hloss, width = barWidth,\n",
    "        edgecolor ='grey', label ='encoded')\n",
    "\n",
    "\n",
    "ax1.legend(fontsize='x-small')\n",
    "\n",
    "plt.ylabel('hamming loss', fontsize = 11)\n",
    "plt.xticks([r + barWidth for r in range(len(vanila_weight))],\n",
    "        ['fold 1', 'fold 2', 'fold 3', 'fold 4', 'fold 5'])\n",
    "fig.suptitle(\"hamming loss comparison\")\n",
    "# fig.tight_layout()\n",
    "display(fig)\n",
    "f_name = \"{}img{}multi_hamm_loss_comp.png\".format(RESULTS, sep)\n",
    "fig.savefig(f_name, bbox_inches =\"tight\",\n",
    "                        pad_inches = 0.15)\n",
    "\n",
    "# close all fig after printed\n",
    "plt.close('all')\n",
    "_i = plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd255dca-68ed-42e9-ae2b-b94e70c15812",
   "metadata": {},
   "source": [
    "### Accuracy Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2faebebb-89d1-4047-afc0-dd1137680540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comaprison on accuracy_score\n",
    "# its using sklearn.metrics.accuracy_score\n",
    "# add foot note higher is better\n",
    "\n",
    "vanila_ascore = []\n",
    "encoded_ascore= []\n",
    "for i in range(5) :\n",
    "    vanila_ascore.append(vanilla_measurement['metric'][i]['accuracy_score'])\n",
    "    encoded_ascore.append(encoded_measurement['metric'][i]['accuracy_score'])\n",
    "\n",
    "_i = plt.ioff()\n",
    "\n",
    "# set width of bar\n",
    "barWidth = 0.25\n",
    "fig, ax1 = plt.subplots(1, 1, figsize=(10,3))\n",
    "\n",
    "# Set position of bar on X axis\n",
    "br1 = np.arange(len(vanila_ascore))\n",
    "br2 = [x + barWidth for x in br1]\n",
    "\n",
    "# Make the plot\n",
    "# ax1.bar(br1, vanila_ascore, width = barWidth,\n",
    "#         edgecolor ='grey', label ='vanila')\n",
    "ax1.bar(br1, vanila_ascore, width = barWidth,\n",
    "        edgecolor ='grey', label ='base')\n",
    "ax1.bar(br2, encoded_ascore, width = barWidth,\n",
    "        edgecolor ='grey', label ='encoded')\n",
    "\n",
    "\n",
    "ax1.legend(fontsize='x-small')\n",
    "\n",
    "plt.ylabel('accuracy score', fontsize = 11)\n",
    "plt.xticks([r + barWidth for r in range(len(vanila_ascore))],\n",
    "        ['fold 1', 'fold 2', 'fold 3', 'fold 4', 'fold 5'])\n",
    "fig.suptitle(\"accuracy score comparison\")\n",
    "# fig.tight_layout()\n",
    "display(fig)\n",
    "\n",
    "f_name = \"{}img{}multi_acc_score_comp.png\".format(RESULTS, sep)\n",
    "fig.savefig(f_name, bbox_inches =\"tight\",\n",
    "                        pad_inches = 0.15)\n",
    "# close all fig after printed\n",
    "plt.close('all')\n",
    "_i = plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f670e395-21fa-418b-8186-3ba7ddbc9bf7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Show Important Table\n",
    "\n",
    "Output a latex, average it for all fold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d66a4fc-6a0f-46bd-9296-cc1ce2ba8aab",
   "metadata": {},
   "source": [
    "## Vanilla"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c979a20d-93d7-4414-bd55-14bedc7815c2",
   "metadata": {},
   "source": [
    "### Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01da85a5-223b-42fb-afb7-2e96ba6929c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_avg = None\n",
    "\n",
    "# sum all dataframe\n",
    "for i in range(len(vanilla_measurement['result'])) :\n",
    "    if i == 0 :\n",
    "        f_avg = vanilla_measurement['result'][i]\n",
    "    else :\n",
    "        f_avg = f_avg.add(vanilla_measurement['result'][i])\n",
    "\n",
    "# divide by amount of fold\n",
    "f_avg = f_avg.div(len(vanilla_measurement['result']))\n",
    "\n",
    "# round down\n",
    "f_avg['TP'] = f_avg['TP'].astype('int')\n",
    "f_avg['FN'] = f_avg['FN'].astype('int')\n",
    "f_avg['FP'] = f_avg['FP'].astype('int')\n",
    "f_avg['TN'] = f_avg['TN'].astype('int')\n",
    "\n",
    "print(f_avg.style.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab516f63-9f26-4c26-8649-e182f012d78d",
   "metadata": {},
   "source": [
    "### Fold\n",
    "\n",
    "Convert fold to latex table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3390e2c1-2368-46b2-9773-bce0d3c2fd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# c40\n",
    "# loop through all dataframe\n",
    "for i in range(len(vanilla_measurement['result'])) :\n",
    "    print(\"Vanilla Fold-{} Tabel\".format(i+1))\n",
    "    print(vanilla_measurement['result'][i].style.to_latex())\n",
    "    # print(vanilla_measurement['result'][i], '\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fa0aab-e89d-40e3-9328-8ac6a7c479f7",
   "metadata": {},
   "source": [
    "### F1 All fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53a03cf-493e-464c-a577-190ca95a39c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through all f1\n",
    "v_micro = []\n",
    "v_macro = []\n",
    "v_weighted = []\n",
    "\n",
    "# v_micro = ['micro']\n",
    "# v_macro = ['macro']\n",
    "# v_weighted = ['weighted']\n",
    "\n",
    "v_colName = []\n",
    "for i in range(len(vanilla_measurement['metric'])) :\n",
    "    v_micro.append(vanilla_measurement['metric'][i]['f1']['micro'])\n",
    "    v_macro.append(vanilla_measurement['metric'][i]['f1']['macro'])\n",
    "    v_weighted.append(vanilla_measurement['metric'][i]['f1']['weighted'])\n",
    "    v_colName.append('fold-{}'.format(i+1))\n",
    "\n",
    "v_micro.append(sum(v_micro)/len(v_micro))\n",
    "v_macro.append(sum(v_macro)/len(v_macro))\n",
    "v_weighted.append(sum(v_weighted)/len(v_weighted))\n",
    "v_colName.append('average')\n",
    "v_f1 = pd.DataFrame({'micro' : v_micro, 'macro' : v_macro, 'weighted' : v_weighted}).T\n",
    "v_f1.columns = v_colName\n",
    "v_f1.index.name='f1'\n",
    "print(v_f1)\n",
    "\n",
    "print(v_f1.style.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9895eaad-f24d-4522-8df7-12a964e6db59",
   "metadata": {},
   "source": [
    "## Encoded Average"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbb46d6-3e15-4344-b362-5a6571660fd1",
   "metadata": {},
   "source": [
    "### Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbafc4e-bc70-42ba-a84b-e61f5a6284bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_avg = None\n",
    "\n",
    "# sum all dataframe\n",
    "for i in range(len(encoded_measurement['result'])) :\n",
    "    if i == 0 :\n",
    "        f_avg = encoded_measurement['result'][i]\n",
    "    else :\n",
    "        f_avg = f_avg.add(encoded_measurement['result'][i])\n",
    "\n",
    "# divide by amount of fold\n",
    "f_avg = f_avg.div(len(encoded_measurement['result']))\n",
    "\n",
    "# round down\n",
    "f_avg['TP'] = f_avg['TP'].astype('int')\n",
    "f_avg['FN'] = f_avg['FN'].astype('int')\n",
    "f_avg['FP'] = f_avg['FP'].astype('int')\n",
    "f_avg['TN'] = f_avg['TN'].astype('int')\n",
    "\n",
    "print(f_avg.style.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54d89af-5215-4b50-9078-5c4865737312",
   "metadata": {},
   "source": [
    "### Fold\n",
    "\n",
    "Convert fold to latex table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076159bc-cbc2-4318-bb48-9c8da9d8a164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through all dataframe\n",
    "for i in range(len(encoded_measurement['result'])) :\n",
    "    print(\"Encoded Fold-{} Tabel\".format(i+1))\n",
    "    print(encoded_measurement['result'][i].style.to_latex())\n",
    "    # print(encoded_measurement['result'][i], '\\n')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07bf0ec-c848-4f54-baeb-4919352fe9f6",
   "metadata": {},
   "source": [
    "### F1 All fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47fdbb4-1c40-4c35-8795-57c822c9b908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through all f1\n",
    "v_micro = []\n",
    "v_macro = []\n",
    "v_weighted = []\n",
    "\n",
    "v_colName = []\n",
    "for i in range(len(encoded_measurement['metric'])) :\n",
    "    v_micro.append(encoded_measurement['metric'][i]['f1']['micro'])\n",
    "    v_macro.append(encoded_measurement['metric'][i]['f1']['macro'])\n",
    "    v_weighted.append(encoded_measurement['metric'][i]['f1']['weighted'])\n",
    "    v_colName.append('fold-{}'.format(i+1))\n",
    "    \n",
    "v_micro.append(sum(v_micro)/len(v_micro))\n",
    "v_macro.append(sum(v_macro)/len(v_macro))\n",
    "v_weighted.append(sum(v_weighted)/len(v_weighted))\n",
    "v_colName.append('average')\n",
    "\n",
    "v_f1 = pd.DataFrame({'micro' : v_micro, 'macro' : v_macro, 'weighted' : v_weighted}).T\n",
    "v_f1.columns = v_colName\n",
    "v_f1.index.name='f1'\n",
    "\n",
    "print('Encoded F1 All')\n",
    "print(v_f1)\n",
    "\n",
    "print(v_f1.style.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0b7f38-c7ee-4c5d-b8c8-a54af2784ba9",
   "metadata": {},
   "source": [
    "## Analisis Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f65f2de-0f23-4ba4-b261-3f2a98707ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "baris = shuff.shape[0]\n",
    "print(\"Jumlah baris pada dataset : {}\".format(baris))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4aaa17-8376-4b9a-b207-628f8969cf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribusi dan jumlah tiap kategori\n",
    "# shuff['label'].tolist()\n",
    "class_dict\n",
    "\n",
    "\n",
    "lbl_ = np.array(shuff['label_cat'].tolist())\n",
    "lbl_ = sum(lbl_, 0)\n",
    "# lbl_ = ???\n",
    "# so we sum it all the lsit to a single list contain count of all label with len 27\n",
    "\n",
    "count_dict = {}\n",
    "for key, item in class_dict.items() :\n",
    "    count_dict[item] = (lbl_[key] / baris) * 100\n",
    "    \n",
    "count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5234fd2a-790e-4878-8aad-79e7964fb636",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_i = plt.ioff()\n",
    "\n",
    "label = list(count_dict.keys())\n",
    "count = list(count_dict.values())\n",
    "\n",
    "fig, ax1 = plt.subplots(1, 1, figsize=(10,6))\n",
    " \n",
    "\n",
    "ax1.barh(label, count, align='center')\n",
    "ax1.set_yticks(label, labels=label)\n",
    "ax1.invert_yaxis()  # labels read top-to-bottom\n",
    "ax1.set_xlabel('Persentase Data')\n",
    "ax1.set_title('Distribusi Label dalam Dataset')\n",
    "\n",
    "plt.show()\n",
    "f_name = \"{}img{}multi_label_distribution.png\".format(RESULTS, sep)\n",
    "fig.savefig(f_name, bbox_inches =\"tight\",\n",
    "                        pad_inches = 0.15)\n",
    "\n",
    "# close all fig after printed\n",
    "plt.close('all')\n",
    "_i = plt.ion()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
